<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on JLING-L</title>
        <link>https://JLING-L.github.io/post/</link>
        <description>Recent content in Posts on JLING-L</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>JLING-L</copyright>
        <lastBuildDate>Tue, 01 Apr 2025 21:01:52 +0800</lastBuildDate><atom:link href="https://JLING-L.github.io/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>大模型笔记 | DPO、PPO</title>
        <link>https://JLING-L.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0-dpoppo/</link>
        <pubDate>Tue, 01 Apr 2025 21:01:52 +0800</pubDate>
        
        <guid>https://JLING-L.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0-dpoppo/</guid>
        <description>&lt;img src="https://JLING-L.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0-dpoppo/po.png" alt="Featured image of post 大模型笔记 | DPO、PPO" /&gt;</description>
        </item>
        <item>
        <title>深度学习基础2 | 权重初始化、正则化、Normalization、优化器</title>
        <link>https://JLING-L.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%802-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E6%AD%A3%E5%88%99%E5%8C%96normalization%E4%BC%98%E5%8C%96%E5%99%A8/</link>
        <pubDate>Tue, 01 Apr 2025 20:51:26 +0800</pubDate>
        
        <guid>https://JLING-L.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%802-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E6%AD%A3%E5%88%99%E5%8C%96normalization%E4%BC%98%E5%8C%96%E5%99%A8/</guid>
        <description>&lt;img src="https://JLING-L.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%802-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E6%AD%A3%E5%88%99%E5%8C%96normalization%E4%BC%98%E5%8C%96%E5%99%A8/normalization.png" alt="Featured image of post 深度学习基础2 | 权重初始化、正则化、Normalization、优化器" /&gt;</description>
        </item>
        <item>
        <title>大模型笔记 | GPT系列</title>
        <link>https://JLING-L.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0-gpt%E7%B3%BB%E5%88%97/</link>
        <pubDate>Sun, 30 Mar 2025 15:21:57 +0800</pubDate>
        
        <guid>https://JLING-L.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0-gpt%E7%B3%BB%E5%88%97/</guid>
        <description>&lt;img src="https://JLING-L.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0-gpt%E7%B3%BB%E5%88%97/gpt.png" alt="Featured image of post 大模型笔记 | GPT系列" /&gt;&lt;h2 id=&#34;gpt-1&#34;&gt;GPT-1
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;Improving Language Understanding by Generative Pre-Training (2018)&lt;/em&gt;&lt;br&gt;
论文：https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&lt;/p&gt;
&lt;p&gt;自然语言理解涉及多种任务，包括文本蕴含、问答、语义相似度评估、文档分类等。尽管有大量无标注语料库可用，但缺少针对这些具体任务的标注数据，导致仅使用监督学习训练的模型难以取得良好效果。&lt;/p&gt;
&lt;p&gt;但数据标注的成本是十分昂贵的，如果一定要依赖监督数据进行训练，那么模型很难快速地迭代。而现实世界中如此丰富的语料库，只能看不能用，也不是办法。因此，改变训练方式，充分利用无标注数据来学习语言信息，是一种高性价比的替代方案。&lt;/p&gt;
&lt;p&gt;但确定好了用无监督学习的方法，还需要考虑两个问题：&lt;br&gt;
(1) 目标函数的不确定性：不同方法在不同任务上的表现存在差异，尚未有统一的最优目标。例如对于机器翻译任务，模型需要学习不同语言间的对齐信息，从而从中获取泛化的语言表示；而对于文本生成任务，模型的目标则是预测下一个单词(或被Mask的单词)。&lt;br&gt;
(2) 迁移学习方法的不确定性：将预训练语言模型迁移到下游任务的方法不统一，已有方法在迁移学习时，通常需要修改模型架构、使用复杂的学习策略或增加辅助学习目标。&lt;/p&gt;
&lt;p&gt;作者提出的方案是，先在大规模的无标注数据上进行预训练，使模型学习通用的表示，此时获得的是一个任务无关的通用模型；然后在目标任务上使用带标注的数据进行微调，使模型可以适配各种目标任务。&lt;/p&gt;
&lt;img src=&#34;gpt1.png&#34; style=&#34;zoom:57%&#34; /&gt;
&lt;h3 id=&#34;model-architecture&#34;&gt;Model Architecture
&lt;/h3&gt;&lt;p&gt;GPT-1的结构主要基于Transformer的decoder部分。经典的Transformer decoder包含Masked Multi-Head Attention，Encoder-Decoder Attention和Feed Forward三个子层。由于不需要encoder，因此只保留Masked Multi-Head Attention, 和Feed Forward两个子层 (每个子层后面都有残差连接和LayerNorm)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为什么选择用Transformer：&lt;/strong&gt;
Transformer允许模型在大规模文本上进行无监督语言建模，且适用于文本生成、分类、翻译、摘要、问答等多种任务。此外，与RNN、CNN等模型相比，Transformer有更强的对长距离依赖的捕捉能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为什么用decoder-only架构：&lt;/strong&gt; GPT-1的任务是语言建模，不需要编码完整的输入句子，只需要逐步预测下一个词，这符合Transformer decoder的特点。&lt;/p&gt;
&lt;h3 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised pre-training
&lt;/h3&gt;$$
L_1(U) = \sum_{i} logP(u_i|u_{i-k},...,u_{i-1};\Theta)
$$&lt;p&gt;
其中，$k$是上下文窗口的大小，即在预测当前token时，模型可以看到前k个token作为上下文信息，$\Theta$是模型的参数(GPT-1的神经网络权重)。使用随机梯度下降法进行训练。&lt;/p&gt;
$$
h_0 = UW_e + W_p
$$$$
h_l = transformer(h_{l-1})
$$$$
P(u) = softmax(h_nW_e^T)
$$&lt;p&gt;
其中，$W_e$表示词嵌入矩阵；$W_p$表示位置嵌入矩阵；$W_e^T$表示线性映射层对应的参数矩阵，维度是词表的大小。&lt;/p&gt;
&lt;h3 id=&#34;supervised-fine-tuning&#34;&gt;Supervised fine-tuning
&lt;/h3&gt;&lt;p&gt;得到无监督预训练模型后，为了使参数适应监督目标任务，可以使用监督数据对模型进行微调。&lt;/p&gt;
$$
P(y|x_1, ..., x_m) = softmax(h_m^lW_y)
$$$$
L_2(C) = \sum_{(x,y)}logP(y|x_1, ..., x_m)
$$$$
L_3(C) = L_2(C) + \lambda·L_1(C)
$$&lt;p&gt;
其中，$\lambda$是一个超参数，用于平衡这两个目标的影响。&lt;/p&gt;
&lt;p&gt;在微调阶段，除了原本的预训练参数外，仅需要额外优化线性输出层的参数$W_y$和分隔符token的嵌入矩阵。&lt;/p&gt;
&lt;h3 id=&#34;task-specific-input-transformations&#34;&gt;Task-specific input transformations
&lt;/h3&gt;&lt;p&gt;对于一些任务，如文本分类，因为它们的输入结构较为简单，可以直接对模型进行微调。而对于问答或文本蕴含等任务，它们的输入包含结构化信息，例如问答任务的输入可能包含文本、问题和多个可能的答案，文本蕴含任务的输入包含前提和假设两个句子。由于预训练模型是在连续文本序列上训练的，因此需要对这些任务的输入做一定的调整，才能使模型适应这些结构化的输入。&lt;/p&gt;
&lt;p&gt;作者采用遍历式的输入转换，将结构化的输入转换为有序序列，使其适应预训练模型的输入方式，从而避免对模型架构做大量的任务特定修改。&lt;/p&gt;
&lt;p&gt;对于不同任务，输入的转换方式如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;文本蕴涵：将前提(premise)和假设(hypothesis)通过分隔符(Delimiter)隔开，两端加起始和中止token，再以此通过Transformer和全连接得到预测结果。&lt;/li&gt;
&lt;li&gt;文本相似：对于需要比较两个句子的任务(如文本相似度任务)，不需要关心两个句子的顺序。为了处理这种情况，将两种可能的句子顺序都转换成输入序列，并在其中加入分隔符，然后分别处理这两个序列，最终将得到的表示通过元素相加的方式合并，再输入一个线性层。&lt;/li&gt;
&lt;li&gt;问答与常识推理：对于这些任务，输入包含文档$z$、问题$q$和可能的答案集${a_k}$。将$z+q$与每个$a$连接，并在每部分之间插入分隔符。然后分别处理每个序列，并通过softmax层得到每个答案的概率分布。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;GPT-1的优势在于运用了大量的无监督数据，并且在网络结构方面，无论输入、输出形式如何，中间的Transformer网络架构都不会改变。&lt;/p&gt;
&lt;h2 id=&#34;gpt-2&#34;&gt;GPT-2
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;Language Models are Unsupervised Multitask Learners (2019)&lt;/em&gt;&lt;br&gt;
论文：https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&lt;/p&gt;
&lt;p&gt;在GPT-1中，已经证明了无监督预训练+监督微调的有效性，但仍然存在一些问题：一方面，GPT-1仍然需要任务特定的数据集进行微调，在零样本(zero-shot)学习能力上仍然有限；另一方面，GPT-1仅使用117M参数，无法充分利用大规模数据进行更深入的学习。&lt;/p&gt;
&lt;p&gt;为此，GPT-2进行了更大规模的训练，尝试提升语言模型在从未见过的任务(zero-shot learning)中的性能表现。&lt;/p&gt;
&lt;img src=&#34;gpt-2.png&#34; style=&#34;zoom:27%&#34; /&gt;
&lt;h3 id=&#34;training-dataset&#34;&gt;Training Dataset
&lt;/h3&gt;&lt;p&gt;过去的语言模型通常是在单一领域的文本上进行训练，虽然这些数据集质量较高，但数据的多样性较低，无法让模型学到广泛的语言任务。GPT-2希望训练数据集尽可能大、涵盖尽可能多的领域和场景，以便让模型学习到广泛的自然语言任务。因此，研究人员创建了WebText：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据来源：Reddit社交平台上的外部链接，该链接至少获得3Karma(点赞)，表面该链接可能包含有趣、教育性或高质量的内容。&lt;/li&gt;
&lt;li&gt;数据清理：使用Dragnet和Newspaper解析HTML页面并提取正文内容。然后进行去重和启发式清理，最后得到800万个文档、共计40GB文本。&lt;/li&gt;
&lt;li&gt;去除了维基百科(Wikipedia)，因为它已经被广泛用于其他数据集，可能会导致测试数据与训练数据重叠，影响评估结果的可靠性。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;model-architecture-1&#34;&gt;Model Architecture
&lt;/h3&gt;&lt;p&gt;GPT-2在GPT-1的基础上进行了一些改进：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Layer Normalization：归一化层(LayerNorm)移至每个子块的输入端，类似Pre-activation ResNet，并在自注意力块后额外增加了一层归一化。&lt;/li&gt;
&lt;li&gt;参数初始化：采用改进的初始化方法，考虑残差路径随深度增加的累积效应。因此残差层的权重在初始化时按$\frac{1}{\sqrt N}$(N为残差层数)的因子进行缩放，以稳定训练。&lt;/li&gt;
&lt;li&gt;词汇表扩展：词汇量从GPT-1的40,000扩展到50,257，以提高对更广泛文本的覆盖能力。&lt;/li&gt;
&lt;li&gt;上下文窗口扩展：上下文长度从512 tokens增加到1024 tokens，增强对长文本的建模能力。&lt;/li&gt;
&lt;li&gt;batch size增大：batch Size增加到512。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;unsupervised-pre-training-1&#34;&gt;Unsupervised pre-training
&lt;/h3&gt;$$
p(x) = \prod_{i=1}^{n} p(s_n \mid s_1, \dots, s_{n-1})
$$$$
p(output|input)
$$$$
p(output|input, task)
$$&lt;p&gt;
而任务的条件化通常有两种方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;架构层面：例如针对不同任务涉及专门的子网络或特定的参数分支。&lt;/li&gt;
&lt;li&gt;算法层面：例如MAML，它使用内外循环优化，使模型能够快速适应新任务。内循环指在当前任务上进行快速微调，外循环指优化整个模型，使其在遇到新任务时能快速适应。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但McCann等人发现，语言本身可以作为一种灵活的、接近自然语言方式来指定任务、输入和输出，例如：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;机器翻译任务可以表示为：(translate to French, English text, French text)&lt;/li&gt;
&lt;li&gt;阅读理解任务可以表示为：(answer the question, document, question, answer)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由于语言建模的目标是预测下一个单词，它本质上可以学习上述任务，而不需要显式标注输出，而互联网本身又包含大量的适合无监督学习的静态文本数据。因此有了以下假设：如果语言模型足够大，它将自动学习如何执行任务，以便更好地预测文本中的下一个单词。换言之，语言模型可能已经在无监督环境中执行了多任务学习。&lt;/p&gt;
&lt;p&gt;GPT-2验证了通过大规模数据训练的模型具有zero-shot能力，而无需额外的微调。并且随着模型参数量的增大，还有进一步提升的空间。&lt;/p&gt;
&lt;h2 id=&#34;gpt-3&#34;&gt;GPT-3
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;Language Models are Few-Shot Learners (2020)&lt;/em&gt;&lt;br&gt;
论文：https://arxiv.org/abs/2005.14165&lt;/p&gt;
&lt;p&gt;GPT-2的虽然有zero-shot learning的能力，但是效果并不好，往往还是需要标签数据进行微调以在某个任务上获得更好的结果。&lt;/p&gt;
&lt;p&gt;在此基础上，GPT-3希望通过扩大模型规模，进一步减少模型对任务特定微调的依赖。GPT-3可以通过在推理时提供少量示例或指令来适应新任务(few-shot learning)。或者说它可以在没有梯度更新的情况下，通过上下文学习(in-context learning)来执行各种任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zero-shot Learning (零样本学习)&lt;/strong&gt;&lt;br&gt;
Zero-shot learning指的是模型在没有见过特定任务的任何训练数据或示例的情况下，通过仅仅依赖任务描述或自然语言指令来执行任务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Few-shot Learning (少样本学习)&lt;/strong&gt;&lt;br&gt;
Few-shot learning指的是模型在仅有少量示例作为输入的情况下进行任务学习，并生成合适的输出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In-context Learning (上下文学习)&lt;/strong&gt;&lt;br&gt;
In-context learning是指模型在推理过程中，通过上下文中的示例来“学习”任务，而不需要传统的微调。它与few-shot learning类似，但着重强调的是任务在输入文本中的上下文信息。&lt;/p&gt;
&lt;img src=&#34;gpt-3.png&#34; style=&#34;zoom:57%&#34; /&gt;
&lt;h3 id=&#34;training-dataset-1&#34;&gt;Training Dataset
&lt;/h3&gt;&lt;p&gt;为了训练GPT-3，对数据集进行了以下处理：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过与高质量参考语料库的相似性对Common Crawl数据集进行过滤，确保数据质量。&lt;/li&gt;
&lt;li&gt;在文档层面去重，确保训练数据不会出现重复内容，同时保持验证集的完整性，以防止过拟合。&lt;/li&gt;
&lt;li&gt;加入了扩展版的WebText、Books1和Books2语料库，及英文维基百科。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;model-architecture-2&#34;&gt;Model Architecture
&lt;/h3&gt;&lt;p&gt;GPT-3的架构与GPT-2类似，但进行了一些改进：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;词嵌入的大小从GPT-2的1600增加到GPT-3的12888。&lt;/li&gt;
&lt;li&gt;上下文窗口从GPT-2的1024增加到GPT-3的2048。&lt;/li&gt;
&lt;li&gt;与GPT-2不同，GPT-3引入了交替的稠密(dense)和局部带状稀疏(locally banded sparse)注意力模式，以减少计算复杂度，同时保留模型的表现。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;稠密注意力模式 (Dense Attention)&lt;/strong&gt;&lt;br&gt;
稠密注意力模式是指每个输入位置都与所有其他位置进行相互注意。它允许每个位置关注整个输入序列，因此模型可以捕捉全局信息，非常适合需要全面上下文信息的任务。但随着输入序列长度的增加，计算量呈二次方增长，复杂度高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;局部带状稀疏注意力模式 (Local Banded Sparse Attention)&lt;/strong&gt;&lt;br&gt;
在稀疏注意力中，每个token只会与其邻近的token进行注意力计算，而不是与所有token计算。它专注于局部信息，计算量大大降低。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;交替模式 (Alternating Attention Pattern)&lt;/strong&gt;&lt;br&gt;
交替模式是指在Transformer层中，某些层使用稠密注意力模式，而其他层使用局部带状稀疏注意力模式。通过这种方式，可以有效地结合全局和局部信息，并同时减少计算开销。&lt;/p&gt;
&lt;p&gt;为了研究模型规模对机器学习表现的影响，作者训练了8个不同规模的模型，模型的规模从1.25亿参数到1750亿参数不等，其中最大规模的模型即为GPT-3，不同大小的模型参数如图所示：&lt;/p&gt;
&lt;img src=&#34;gpt3-table.png&#34; style=&#34;zoom:57%&#34; /&gt;
&lt;p&gt;GPT-3在Zero-shot、Few-shot和One-shot上表现优异，尤其是在Few-shot场景下。且GPT-3展示了其非常强的泛化能力，能够通过少量示例来适应新任务，显著降低了对任务特定数据的依赖。此外，GPT-3能够仅通过自然语言的任务描述来理解和执行任务，进一步减少了手动工程设计和复杂输入的需求。&lt;/p&gt;
&lt;h2 id=&#34;instructgpt&#34;&gt;InstructGPT
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;Training language models to follow instructions with human feedback (2022)&lt;/em&gt;&lt;br&gt;
论文：https://arxiv.org/abs/2203.02155&lt;/p&gt;
&lt;p&gt;GPT-3在生成文本时，可能会做出不符合用户期望的行为，例如编造事实、生成有偏见或有毒的文本，或者根本不遵循用户指令。这是因为模型的训练目标是预测下一个词，而不是“帮助用户完成任务”和“安全地遵循用户的指令”。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，OpenAI提出了InstructGPT，它通过人类反馈和强化学习调整了模型的行为，使其更好地遵循指令和符合用户意图，变得有帮助(helpful)、诚实(honest)和无害(harmless)。&lt;/p&gt;
&lt;h3 id=&#34;high-level-methodology&#34;&gt;High-level methodology
&lt;/h3&gt;&lt;img src=&#34;instructgpt.png&#34; style=&#34;zoom:57%&#34; /&gt;
&lt;p&gt;InstructGPT参考了Ziegler et al.(2019)和 Stiennon et al.(2020)的工作。通过使用人类反馈强化学习(RLHF)，使模型的输出更加符合用户的意图。该方法首先选择一个预训练的语言模型(如GPT-3)，并确定一个包含各种输入提示的分布。然后，组一个经过训练的标注员团队，他们需要提供所需的行为示范和反馈。方法按以下步骤执行：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;收集示范数据并进行监督学习：通过标注员在指定的输入提示分布上提供“行为示范”。这些示范说明了标注员希望模型生成的行为或输出。然后，使用这些示范数据对预训练的GPT-3模型进行监督学习，使其能够根据这些示范数据生成期望的输出。&lt;/li&gt;
&lt;li&gt;收集比较数据并训练奖励模型：标注员将比较多个模型生成的不同输出，标记出哪个输出更符合他们的偏好。基于这些比较数据，训练一个奖励模型(RM)。这个模型的任务是预测哪一个模型输出更符合人类标注员的偏好。&lt;/li&gt;
&lt;li&gt;PPO优化策略：RM的输出是一个标量值(奖励)，这个奖励反映了模型输出与标注员偏好的匹配程度。使用PPO算法对监督策略进行强化学习，目的是优化模型的输出，使得输出的奖励尽可能高(即更符合人类偏好)。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;步骤2和3可以迭代执行，随着强化学习过程的进行，不断收集新的比较数据，用这些数据训练新的奖励模型，再用这些新的奖励模型对模型进行进一步的优化。&lt;/p&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset
&lt;/h3&gt;&lt;p&gt;根据上述的方法过程，可以看到总共需要三种类型的数据集：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SFT数据集：包括标注员提供的示范数据，用于训练监督学习模型(SFT)。&lt;/li&gt;
&lt;li&gt;RM数据集：包括标签员对模型输出的排名数据(比较两个模型的输出，标记哪个输出更符合人类偏好)，用于训练奖励模型(RM)。&lt;/li&gt;
&lt;li&gt;PPO数据集：不含人类标签数据，用于强化学习(RLHF)微调。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中，SFT数据集有约13k条提示，RM数据集有33k条提示，PPO数据集有31k条提示。数据集的提示类型涉及生成、问答、对话、摘要、抽取等多种自然语言任务。&lt;/p&gt;
&lt;h3 id=&#34;models&#34;&gt;Models
&lt;/h3&gt;&lt;p&gt;包括三种主要的训练技术：监督微调(SFT)、奖励模型(RM)和强化学习(RL)。&lt;/p&gt;
&lt;h4 id=&#34;监督微调-sft&#34;&gt;监督微调 (SFT)
&lt;/h4&gt;&lt;p&gt;SFT阶段，使用标注员提供的示范数据对GPT-3进行监督学习微调。基于奖励模型(RM)在验证集上的表现选择最好的SFT模型。&lt;/p&gt;
&lt;h4 id=&#34;reward-modeling-rm&#34;&gt;Reward modeling (RM)
&lt;/h4&gt;&lt;p&gt;RM基于SFT模型进行，去除了最终的解嵌层(用于将词嵌入转换回模型的输出空间)后进行训练。该模型接受一个提(prompt)和一个响应(response)，并输出一个标量奖励值，该值表示给定响应的质量。&lt;/p&gt;
&lt;p&gt;这里仅使用了6B参数的奖励模型，因为175B奖励模型训练时需要的计算资源非常高且不稳定，因此不适合作为强化学习(RL)中的价值函数。&lt;/p&gt;
$$
\text{loss}(\theta) = - \frac{1}{\binom{K}{2}} \mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \left( \sigma \left( r_\theta(x, y_w) - r_\theta(x, y_l) \right) \right) \right]
$$&lt;p&gt;
其中，$r_\theta(x, y)$是奖励模型对提示$x$和响应$y$的标量输出；$y_w$是比较中更好的响应，$y_l$是另一个相应；$D$是包含人类比较数据的数据集；$\sigma$是Sigmoid函数，用于将差值转换为概率。&lt;/p&gt;
&lt;p&gt;为了加速数据收集，标签员每次会看到4~9个响应来进行排名，这样每个提示就会有多个候选响应。对于每个提示，标注员会
从$K$个响应中选择两个进行比较，如果$k=4$，就会有$4×(4-1)/2=6$个比较。&lt;/p&gt;
&lt;p&gt;由于每个任务中的比较数据是高度相关的，研究发现如果直接将所有比较混合到一个数据集中，经过一次训练会导致奖励模型过拟合。因此，将每个提示的所有比较作为一个批次进行训练。&lt;/p&gt;
&lt;h4 id=&#34;reinforcement-learning-rl&#34;&gt;Reinforcement learning (RL)
&lt;/h4&gt;&lt;p&gt;使用PPO算法对SFT模型进行强化学习环境中的微调。使用的环境是一个bandit环境，该环境会随机提供一个客户输入的prompt，并期望模型生成相应的回答。收到prompt并生成回答后，使用RM对生成的回答进行评估，并提供奖励。为了避免奖励模型过度优化并保持原始模型的完整性，在每个token添加KL惩罚。&lt;/p&gt;
$$
\text{objective}(\varphi) = \mathbb{E}_{(x,y) \sim D_{\pi_{\text{RL}}}} \left[ r_{\theta}(x, y) - \beta \log \left( \frac{\pi_{\text{RL}}(y | x)}{\pi_{\text{SFT}}(y | x)} \right) \right] + \gamma \mathbb{E}_{x \sim D_{\text{pretrain}}} \left[ \log \left( \pi_{\text{RL}}(x) \right) \right]
$$&lt;p&gt;
强化学习训练的目标函数旨在平衡：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最大化奖励模型预测的奖励$(r_{\theta}(x, y))$;&lt;/li&gt;
&lt;li&gt;最小化强化学习策略与监督训练策略之间的差异$(\beta \log \left( \frac{\pi_{\text{RL}}(y | x)}{\pi_{\text{SFT}}(y | x)} \right))$，其中$\beta$为KL惩罚系数；&lt;/li&gt;
&lt;li&gt;通过预训练损失项保持预训练数据的分布。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;InstructGPT证明了通过细化训练，尤其是使用人类反馈强化学习，能够显著改善语言模型在多个任务上的表现，使其更加符合用户的意图。&lt;/p&gt;
&lt;h2 id=&#34;chatgpt&#34;&gt;ChatGPT
&lt;/h2&gt;&lt;p&gt;InstructGPT在训练时以对齐指令为主，模型的对话能力、逻辑推理能力还需要进一步的改进，ChatGPT(2022.11)是AI对话的一个里程碑。&lt;/p&gt;
&lt;img src=&#34;chatgpt.jpg&#34; style=&#34;zoom:37%&#34; /&gt;
&lt;h2 id=&#34;gpt-4&#34;&gt;GPT-4
&lt;/h2&gt;&lt;p&gt;技术报告：https://arxiv.org/abs/2303.08774&lt;br&gt;
相比基于GPT-3.5的ChatGPT，GPT-4(2023.3)支持多模态输入(可以理解文本、图片)，其理解能力和推理能力也进一步增强。&lt;/p&gt;
&lt;h2 id=&#34;更多版本&#34;&gt;更多版本
&lt;/h2&gt;&lt;p&gt;GPT-4 Turbo (2023.11)&lt;br&gt;
知识库更新至2023.4，上下文窗口扩大至128k，性能进一步提升。&lt;/p&gt;
&lt;p&gt;GPT-4o (2024.05)
能够处理文本、图像、音频输入，生成速度提升2倍，成本降低50%。但逻辑推理能力相对较弱。&lt;/p&gt;
&lt;p&gt;o1-preview&lt;br&gt;
o1预览版本，针对复杂推理任务进行优化。&lt;/p&gt;
&lt;p&gt;o1-mini&lt;br&gt;
轻量版的o1，成本更低、响应更快。&lt;/p&gt;
&lt;p&gt;GPT-o1
逻辑推理能力强，特别是数学和编程，仅支持文本交流。&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference
&lt;/h2&gt;&lt;p&gt;[1] &lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/680022511&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://zhuanlan.zhihu.com/p/680022511&lt;/a&gt;&lt;br&gt;
[2] &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/weixin_65514978/article/details/141018827?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522307bae430be83adb7b0193fef212489a%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;amp;request_id=307bae430be83adb7b0193fef212489a&amp;amp;biz_id=0&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-141018827-null-null.142%5ev102%5epc_search_result_base9&amp;amp;utm_term=gpt%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93&amp;amp;spm=1018.2226.3001.4187&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/weixin_65514978/article/details/141018827?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522307bae430be83adb7b0193fef212489a%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=307bae430be83adb7b0193fef212489a&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-141018827-null-null.142^v102^pc_search_result_base9&amp;utm_term=gpt%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93&amp;spm=1018.2226.3001.4187&lt;/a&gt;&lt;br&gt;
[3] &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/tMb8Z9Vdm66wH68VX1/article/details/128928143?ops_request_misc=%257B%2522request%255Fid%2522%253A%252298ce02c531403ce60b26d74079c9b284%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;amp;request_id=98ce02c531403ce60b26d74079c9b284&amp;amp;biz_id=0&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-128928143-null-null.142%5ev102%5epc_search_result_base9&amp;amp;utm_term=InstructGPT&amp;amp;spm=1018.2226.3001.4187&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/tMb8Z9Vdm66wH68VX1/article/details/128928143?ops_request_misc=%257B%2522request%255Fid%2522%253A%252298ce02c531403ce60b26d74079c9b284%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=98ce02c531403ce60b26d74079c9b284&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-128928143-null-null.142^v102^pc_search_result_base9&amp;utm_term=InstructGPT&amp;spm=1018.2226.3001.4187&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>深度学习基础1 | 感知机、激活函数</title>
        <link>https://JLING-L.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%801-%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</link>
        <pubDate>Sun, 30 Mar 2025 15:21:57 +0800</pubDate>
        
        <guid>https://JLING-L.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%801-%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</guid>
        <description>&lt;img src="https://JLING-L.github.io/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%801-%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/deep_neural_network.png" alt="Featured image of post 深度学习基础1 | 感知机、激活函数" /&gt;&lt;h2 id=&#34;神经元&#34;&gt;神经元
&lt;/h2&gt;&lt;p&gt;神经网络(Neural Network)的起源可以追溯到20世纪40年代，沃伦·麦卡洛克(Warren McCulloch)和沃尔特·皮茨(Walter Pitts)两位学者提出了MP神经元，这一数学模型模仿了生物神经元的激活模式。&lt;/p&gt;
&lt;img src=&#34;neurons.png&#34; style=&#34;zoom:53%&#34; /&gt;  
$$
z = g(a_1 * W_1 + a_2 * W_2 + a_3 * W_3)
$$$$
\text{sgn}(x) =
\begin{cases} 
1, &amp; x \geq 0 \\
0, &amp; x &lt; 0
\end{cases}
$$&lt;p&gt;
其中的x表示加权输入的总和，当输入总和$\geq0$时，神经元输出1(被激活)，否则输出0(未激活)。&lt;/p&gt;
&lt;img src=&#34;neurons_1.png&#34; style=&#34;zoom:53%&#34; /&gt;
&lt;p&gt;将sum、sgn绘制在一起表示神经元的内部计算。单个神经元可以引出多个相同值的有向箭头。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;神经元模型的目标是什么？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;神经元的目标是学习一个最优的计算规则，让输入数据经过它后，能尽可能准确地预测输出。&lt;br&gt;
可以将神经元视作一个决策器，比如决定今天是否要带伞：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;输入：天气预报的温度、湿度、云量、风速等；&lt;/li&gt;
&lt;li&gt;权重：不同特征对决策的影响程度；&lt;/li&gt;
&lt;li&gt;计算(加权求和+激活)：综合所有信息后，决定是否带伞；&lt;/li&gt;
&lt;li&gt;输出：最终决定(如 1=带伞，0=不带伞)。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;局限性&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在前面的内容中可以发现，这一方法存在一些局限性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;只支持二值输出：由于使用符号函数进行决策，神经元的输出为0或1；&lt;/li&gt;
&lt;li&gt;无法处理非线性问题：sgn函数是线性的，它只会根据加权和是否大于0来判断；&lt;/li&gt;
&lt;li&gt;不能自我学习：所有权重和阈值均需要人工设计。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;单层感知机-单层神经网络&#34;&gt;单层感知机 (单层神经网络)
&lt;/h2&gt;&lt;p&gt;1958年，Frank Rosenblatt提出了感知机模型，通过引入学习算法，使得神经网络能够通过训练调整权重，而无需人工设计。&lt;br&gt;
感知机的目标是通过不断调整权重，使模型能够更好地预测或分类。&lt;/p&gt;
&lt;img src=&#34;perceptron.png&#34; style=&#34;zoom:53%&#34; /&gt;   
&lt;p&gt;在神经元模型的“输入”位置添加神经元节点，标志其为“输入单元”。&lt;br&gt;
感知机中有两个层次，分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算；输出层里的“输出单元”则需要对前面一层的输入进行计算。&lt;/p&gt;
&lt;img src=&#34;perceptron_1.png&#34; style=&#34;zoom:33%&#34; /&gt;  
$$
z_1 = g(a_1 * W_1 + a_2 * W_2 + a_3 * W_3)
$$$$
z_2 = g(a_1 * W_4 + a_2 * W_5 + a_3 * W_6)
$$$$
z_1 = g(a_1 * W_{1,1} + a_2 * W_{1,2} + a_3 * W_{1,3})
$$$$
z_2 = g(a_1 * W_{2,1} + a_2 * W_{2,2} + a_3 * W_{2,3})
$$$$
g(W*a) = z
$$&lt;p&gt;
其中，$a=[a_1,a_2,a_3]^T$；$z=[z_1,z_2]^T$；W为2行3列的矩阵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;感知机是如何训练的？&lt;/strong&gt;&lt;/p&gt;
$$
f(x) = sign(w^T·x+b)
$$$$
sign(x) =
\begin{cases} 
+1, &amp; x \geq 0 \\
-1, &amp; x &lt; 0
\end{cases}
$$&lt;p&gt;&lt;strong&gt;感知机的几何解释&lt;/strong&gt;：线性方程$w·x+b=0$对应于特征空间中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超超平面的截距，这个超平面将特征空间划分为正负两部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;当$w^T·x_i+b\geq0$时，$y_i=+1$；&lt;/li&gt;
&lt;li&gt;当$w^T·x_i+b&amp;lt;0$时，$y_i=-1$；&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&#34;perceptron_2.png&#34; style=&#34;zoom:33%&#34; /&gt;
$$
y_i(w^T·x_i+b)&lt;0
$$$$
L(w, b) = - \sum_{i \in \mathcal{M}} y_i (w^T· x_i + b)
$$$$
\nabla_w L = - \sum_{i \in \mathcal{M}} y_i x_i
$$$$
\nabla_b L = - \sum_{i \in \mathcal{M}} y_i
$$$$
w = w - \eta \nabla_w L
$$$$
b = b - \eta \nabla_b L
$$$$
w = w + \eta y_ix_i
$$$$
b = b + \eta y_i
$$&lt;p&gt;
其中，$\eta (0&amp;lt;\eta\leq 1)$为步长(学习率)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;什么是梯度？&lt;/strong&gt;&lt;/p&gt;
$$
\nabla_\theta L = (\frac{\partial L}{\partial \theta_1}, \frac{\partial L}{\partial \theta_2}, ..., \frac{\partial L}{\partial \theta_n})
$$$$
\theta \leftarrow \theta-\eta\nabla_\theta L
$$&lt;p&gt;&lt;strong&gt;单层感知机代码&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;Perceptron&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;input_dim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;input_dim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;activation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sign&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;predict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;activation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;errors&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xi&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;yi&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;activation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xi&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;yi&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;yi&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;yi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                    &lt;span class=&#34;n&#34;&gt;errors&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;errors&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 训练&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;perceptron&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Perceptron&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;input_dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;perceptron&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 测试&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;perceptron&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;predict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;预测结果：&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;局限性&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;单层感知机只能处理可以通过一条直线(或超平面)分开的数据。非线性问题(如异或问题)就无法解决。&lt;/p&gt;
&lt;h2 id=&#34;多层感知机-多层神经网络&#34;&gt;多层感知机 (多层神经网络)
&lt;/h2&gt;&lt;p&gt;单层神经网络无法解决非线性问题(如异或问题)，但如果增加一个计算层，通过多次非线性映射，输入数据就可以被映射到更高维的特征空间，从而解决非线性可分问题。这个增加的计算层被称为隐藏层。增加的隐藏层由图中黄色的块表示：&lt;/p&gt;
&lt;img src=&#34;perceptron_3.png&#34; style=&#34;zoom:43%&#34; /&gt;
$$
h_1 = f(w_{1,1}*x_1 + w_{2,1}*x_2 + w_{3,1}*x_3 + b_1)
$$$$
h_2 = f(w_{1,2}*x_1 + w_{2,2}*x_2 + w_{3,3}*x_3 + b_2)
$$$$
y = v_1*h_1 + v_2*h_2 + b
$$&lt;p&gt;
如果$f(·)$是线性的，增加了隐藏层的多层感知机本质上还是一个线性模型。而多层感知机的目的是让模型可以处理非线性问题，因此，这里的$f(·)$需要使用非线性函数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为什么使用非线性激活函数就可以增强模型能力？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;任意非线性函数都可以通过多个分段线性函数的组合来进行近似和表示。而在多层感知机中，激活函数(如ReLU、sigmoid等)帮助神经网络实现了这种分段线性变换，从而使得它可以拟合任意的非线性关系。&lt;/p&gt;
&lt;p&gt;然而，多层神经网络需要处理多个层次的权重和偏置，在每一层之间进行矩阵乘法和激活函数的非线性变换。随着网络深度的增加，计算复杂度呈指数级增长。&lt;/p&gt;
&lt;p&gt;1986年，Rumelhar和Hinton等人提出了反向传播(Backpropagation，BP)算法，解决了两层神经网络所需要的复杂计算量问题。反向传播的核心思想是通过计算损失函数相对于网络各个参数的梯度，来更新神经网络中的权重和偏置，从而使得网络在训练过程中最小化损失。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;链式法则&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于$f(g(x))$，导数为：$\frac{df}{dx} = \frac{df}{dg}·\frac{dg}{dx}$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前向传播&lt;/strong&gt;&lt;/p&gt;
$$
h = \sigma(z_1)
$$$$
z_1 = w_1x+b_1
$$$$
y = z_2
$$$$
z_2 = w_2h+b_2
$$$$
L = \frac{1}{2}(y-t)^2
$$&lt;p&gt;&lt;strong&gt;反向传播&lt;/strong&gt;&lt;/p&gt;
$$
\frac{\alpha L}{\alpha y} = y-t
$$$$
\frac{\alpha L}{\alpha w_2} = \frac{\alpha L}{\alpha y}·\frac{\alpha y}{\alpha w_2} = (y-t)·h^T
$$$$
\frac{\alpha L}{\alpha b_2} = \frac{\alpha L}{\alpha y}·\frac{\alpha y}{\alpha b_2} = y-t
$$$$
\frac{\alpha L}{\alpha h} = \frac{\alpha L}{\alpha y}·\frac{\alpha y}{\alpha h} = (y-t)·W_2^T
$$$$
\frac{\alpha L}{\alpha z_1} = \frac{\alpha L}{\alpha h}·\frac{\alpha h}{\alpha z_1} = \frac{\alpha L}{\alpha h}·\sigma&#39;(z_1)
$$$$
\frac{\alpha L}{\alpha w_1} = \frac{\alpha L}{\alpha z_1}·\frac{\alpha z_1}{\alpha w_1} = \frac{\alpha L}{\alpha z_1}·x^T
$$$$
\frac{\alpha L}{\alpha b_1} = \frac{\alpha L}{\alpha z_1}·\frac{\alpha z_1}{\alpha b_1} = \frac{\alpha L}{\alpha z_1}
$$$$
\frac{\alpha L}{\alpha w_l} = \frac{\alpha L}{\alpha z_l}·\frac{\alpha z_l}{\alpha w_l}
$$$$
\frac{\alpha L}{\alpha b_l} = \frac{\alpha L}{\alpha z_l}·\frac{\alpha z_l}{\alpha b_l}
$$&lt;p&gt;
其中，$w_l$、$b_l$是第$l$层的参数；$z_l$为第$l$层的输入(未激活)；$\frac{\alpha L}{\alpha z_l}$是对第$l$层输入的梯度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多层感知机代码&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;41
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;42
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;43
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;44
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;45
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;46
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;47
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;48
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;49
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;50
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;51
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;52
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;53
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;54
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;55
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;56
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;57
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;58
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;59
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;60
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;61
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;62
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;63
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;64
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;65
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;66
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;MLP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;input_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hidden_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;input_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;input_size&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hidden_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hidden_size&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;input_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hidden_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hidden_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hidden_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sigmoid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sigmoid_derivative&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;maximum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;relu_derivative&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;float&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hidden_input&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hidden_output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hidden_input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;final_input&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hidden_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;final_output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sigmoid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;final_input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;final_output&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;backward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;error&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;final_output&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;error&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sigmoid_derivative&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;final_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;d_hidden&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relu_derivative&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hidden_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hidden_output&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdims&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d_hidden&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d_hidden&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keepdims&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;error&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;epochs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;epoch&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;epochs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;epoch&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Epoch &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;epoch&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;, Loss: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.4f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;predict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 训练&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mlp&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MLP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;input_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hidden_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mlp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 测试&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Final Predictions:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mlp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;predict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;激活函数&#34;&gt;激活函数
&lt;/h2&gt;&lt;p&gt;在前面的内容中，我们多次提到了激活函数这个概念。激活函数在隐藏层和输出层增加了非线性操作，允许网络复制复杂的非线性行为，从而帮助网络学习数据中的复杂模式、提升表达能力。&lt;br&gt;
由于绝大多数神经网络都是借助某种形式的梯度下降进行优化，因此激活函数需要是可微分(或者至少是几乎完全可微分的)。&lt;br&gt;
下面简单介绍一些常用的激活函数。&lt;/p&gt;
$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$$$
\sigma&#39;(x) = \frac{e^{-x}}{(1+e^{-x})^2} = \sigma(x)[1-\sigma(x)]
$$&lt;img src=&#34;sigmoid.png&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;由图可以看出，Sigmoid函数的值域为[0,1]，输出范围有限，优化稳定，可以用作输出层，例如用于表示二分类的类别或者用于表示置信度。&lt;/li&gt;
&lt;li&gt;该函数是连续可导的，可以提供非常平滑的梯度值，防止模型训练过程中出现突变的梯度(即避免跳跃的输出值)。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;导数的最大值只有0.25，网络越深，多个0.25相乘，梯度逐渐趋近于0，反向传播时其权重几乎得不到更新，从而使得模型变得难以训练，这就是&lt;strong&gt;梯度消失&lt;/strong&gt;问题。&lt;/li&gt;
&lt;li&gt;其输出&lt;strong&gt;不是以0为中心而是全大于0的(这会降低权重更新的效率)&lt;/strong&gt;，下一层的神经元会得到上一层输出的全正信号作为输入，因此Sigmoid激活函数一般放在最后的输出层中使用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;指数运算&lt;/strong&gt;耗时。&lt;/li&gt;
&lt;/ol&gt;
$$
f(x) = \frac{e^{x_i}}{\sum_{i=0}^{n}e^{x_i}}
$$&lt;img src=&#34;softmax.jpg&#34; style=&#34;zoom:63%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Softmax将输出转化为概率，所有输出的值都在[0, 1]之间，并且所有类别的概率之和为1，适合多分类任务。&lt;/li&gt;
&lt;li&gt;经过使用指数形式的Softmax函数能够将差距大的数值距离拉的更大。这有助于神经网络模型在进行多分类时区分不同类别的相对优先级。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;需要对所有类别的指数求和，这在类别数很大的时候会导致较高的计算开销。&lt;/li&gt;
&lt;li&gt;在输出层，Softmax函数的梯度非常依赖于模型的输出。当某个类别的输出远大于其他类别时，梯度会变得非常小，导致梯度消失问题。&lt;/li&gt;
&lt;li&gt;容易受到异常值的影响，尤其是在某些类别的得分非常大的时候。一个非常大的输入值可能会导致Softmax输出的概率几乎集中在一个类别上，从而影响其他类别的学习。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Sigmoid与Softmax的区别&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Sigmoid适用于二分类问题 或多标签分类问题，输出为单一的概率值(通常为每个类别的独立概率)，而不是多个类别的相对概率分布。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Softmax适用于多分类问题，它将输出转换为概率分布，并且确保所有类别的概率之和为 1。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(3) Tanh激活函数&lt;/strong&gt;&lt;/p&gt;
$$
tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$$$
tanh&#39;(x) = \frac{4}{(e^x+e^{-x})^2} = 1-[tanh(x)]^2
$$&lt;img src=&#34;tanh.png&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;值域为[-1, 1]，对称中心在原点，解决了Sigmoid激活函数输出不以0为中心的问题。&lt;/li&gt;
&lt;li&gt;可以用于隐藏层。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在两边与Sigmoid一样有梯度趋近于0的问题。&lt;/li&gt;
&lt;li&gt;存在梯度消失问题。&lt;/li&gt;
&lt;li&gt;同样需要进行指数计算。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(4) ReLU激活函数&lt;/strong&gt;&lt;/p&gt;
$$
f(x) =
\begin{cases} 
x, &amp; x \geq 0 \\
0, &amp; x &lt; 0
\end{cases}
$$$$
f&#39;(x) =
\begin{cases} 
1, &amp; x \geq 0 \\
0, &amp; x &lt; 0
\end{cases}
$$&lt;img src=&#34;relu.png&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;正输入时是线性的，收敛速度快(梯度恒为1)，计算速度快。且当输入为正时，由于导数是1，能够完整传递梯度，不存在梯度饱和、梯度消失的问题。&lt;/li&gt;
&lt;li&gt;无论是函数还是其导数都不包含复杂的数学运算，计算复杂度低，只需要一个阈值就可以得到激活值，收敛速度比sigmoid和tanh更快。&lt;/li&gt;
&lt;li&gt;ReLU会使一部分神经元的输出为0，这样就造成了神经网络的稀疏性，并减少了参数的相互依存关系，缓解了过拟合问题的发生。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;输入为负时，输出为0，梯度也为0，相当于神经元直接死亡(Dead Neuron)，而且不会复活，这可能导致特征的学习不充分。&lt;/li&gt;
&lt;li&gt;与Sigmoid一样，其输出不是以0为中心的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(5) Leaky ReLU激活函数&lt;/strong&gt;&lt;/p&gt;
$$
f(x) =
\begin{cases} 
x, &amp; x \geq 0 \\
\lambda x, &amp; x &lt; 0
\end{cases}, \lambda \in (0, 1)
$$$$
f&#39;(x) =
\begin{cases} 
1, &amp; x \geq 0 \\
\lambda, &amp; x &lt; 0
\end{cases}, \lambda \in (0, 1)
$$&lt;img src=&#34;leaky_relu.png&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;通过在负半轴添加一个小的正斜率，使得负轴的信息不会全部丢失，以此解决神经元死亡的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;超参数$\lambda$需要人为设定，但不同任务可能需要不同的最佳$\lambda$，增加了超参数调优的复杂度。&lt;/li&gt;
&lt;li&gt;当输入值较大时，Leaky ReLU仍可能导致梯度爆炸，需要配合Batch Normalization或其他正则化方法。&lt;/li&gt;
&lt;li&gt;在实际操作中，尚未完全证明Leaky ReLU总是比ReLU更好。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(6) PReLU激活函数&lt;/strong&gt;&lt;/p&gt;
$$
f(x) =
\begin{cases} 
x, &amp; x \geq 0 \\
\alpha x, &amp; x &lt; 0
\end{cases}
$$$$
f&#39;(x) =
\begin{cases} 
1, &amp; x \geq 0 \\
\alpha, &amp; x &lt; 0
\end{cases}
$$&lt;p&gt;参数α通常为0到1之间的数字，并且通常相对较小:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果$𝛼 = 0$，则$f(x)$变为ReLU。&lt;/li&gt;
&lt;li&gt;如果$𝛼 &amp;gt; 0$，则$f(x)$变为Leaky ReLU。&lt;/li&gt;
&lt;li&gt;如果$𝛼$是可学习的参数，则$f(x)$为PReLU。&lt;/li&gt;
&lt;/ol&gt;
&lt;img src=&#34;prelu.png&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;与Leaky ReLU类似，PReLU在负值域也有一个相对较小的斜率，可以避免Dead ReLU问题。&lt;/li&gt;
&lt;li&gt;公式与Leaky ReLu相似，但并不完全一样。$𝛼$可以是常数，或自适应调整的参数。也就是说，如果让$𝛼$自适应，那么PReLu会在反向传播时更新参数$𝛼$。因此在不同数据集上，PReLU通常比Leaky ReLU更稳定。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PReLU让每个神经元都学习一个额外的参数$𝛼$，这会增加模型的自由度，导致模型在小数据集上可能过拟合。&lt;/li&gt;
&lt;li&gt;在某些简单任务中，ReLU已经足够有效，而PReLU的额外参数反而可能带来不必要的复杂度。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(7) RReLU激活函数&lt;/strong&gt;&lt;/p&gt;
$$
f(x) =
\begin{cases} 
x, &amp; x \geq 0 \\
\alpha x, &amp; x &lt; 0
\end{cases}
$$$$
f&#39;(x) =
\begin{cases} 
1, &amp; x \geq 0 \\
\alpha, &amp; x &lt; 0
\end{cases}
$$&lt;img src=&#34;rrelu.png&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;与Leaky ReLU类似，RReLU在负半轴保持非零梯度，避免ReLU的神经元死亡问题。&lt;/li&gt;
&lt;li&gt;训练时随机选取$\alpha$，可以一定程度上减少过拟合，提高模型的泛化能力。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;参数$𝛼$取值随机，训练过程中模型参数的更新方向可能会有所波动，在某些任务上可能会影响收敛速度。&lt;/li&gt;
&lt;li&gt;在测试时使用固定的$𝛼$(取均值)，而训练时是随机的，可能导致模型表现略有偏差。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(8) ELU激活函数&lt;/strong&gt;&lt;/p&gt;
$$
f(x) =
\begin{cases} 
x, &amp; x \geq 0 \\
\alpha (e^x-1), &amp; x &lt; 0
\end{cases}
$$$$
f&#39;(x) =
\begin{cases} 
1, &amp; x \geq 0 \\
\alpha e^x, &amp; x &lt; 0
\end{cases}
$$&lt;img src=&#34;elu.png&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ELU在负值时是一个指数函数，具有软饱和特性，对噪声更鲁棒，抗干扰能力强；在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。&lt;/li&gt;
&lt;li&gt;右侧线性部分使ELU能够缓解梯度消失。&lt;/li&gt;
&lt;li&gt;ELU的输出均值接近于0，收敛速度快。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在输入为负时需要进行指数计算，计算稍复杂。&lt;/li&gt;
&lt;li&gt;$\alpha$参数需要额外调整。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(9) SELU激活函数&lt;/strong&gt;&lt;/p&gt;
$$
f(x) =
\lambda\begin{cases} 
x, &amp; x &gt; 0 \\
\alpha (e^x-1), &amp; x \leq 0
\end{cases}
$$$$
f&#39;(x) =
\begin{cases} 
\lambda, &amp; x &gt; 0 \\
\lambda \alpha e^x, &amp; x \leq 0
\end{cases}
$$&lt;img src=&#34;selu.png&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SELU具有自归一化的特性，这使得网络的每一层的输出会在训练过程中保持均值为0，方差为1，从而减轻了梯度消失和梯度爆炸问题。让网络自动保持输入分布的一致性，避免额外的Batch Normalization或Layer Normalization操作保持输出层的稳定。&lt;/li&gt;
&lt;li&gt;内部归一化比外部归一化更快，这使得网络收敛得更快。且每一层的激活函数输出不再依赖于外部约束，减少了对优化器的依赖。&lt;/li&gt;
&lt;li&gt;负值范围通过指数函数保持非零梯度，防止了梯度消失。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SELU本身已经具备了自归一化特性，它不适合与Batch Normalization结合使用。如果同时使用BN，可能会破坏其自归一化能力。&lt;/li&gt;
&lt;li&gt;SELU的指数部分在负值区域导致输出非常小，可能会导致过度的稀疏性。类似神经元长时间“死亡”。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(10) CELU激活函数&lt;/strong&gt;&lt;/p&gt;
$$
f(x) =
\lambda\begin{cases} 
x, &amp; x &gt; 0 \\
\alpha (e^\frac{x}{\alpha}-1), &amp; x \leq 0
\end{cases}
$$$$
f&#39;(x) =
\begin{cases} 
1, &amp; x &gt; 0 \\
e^\frac{x}{\alpha}, &amp; x \leq 0
\end{cases}
$$&lt;img src=&#34;celu.png&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;CELU在负区间使用了平滑的指数函数，梯度更连续、平滑。&lt;/li&gt;
&lt;li&gt;对负值区间的指数部分进行平滑处理，避免了激活值快速增大的问题，不容易引起梯度爆炸。&lt;/li&gt;
&lt;li&gt;负值范围输出非零梯度，防止了梯度消失。&lt;/li&gt;
&lt;li&gt;这种平滑的激活函数使得训练更稳定，有助于深度神经网络的收敛。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在面对复杂的非线性问题时。过度平滑可能会限制网络学习到更复杂的特征表示，从而影响网络的表达能力。&lt;/li&gt;
&lt;li&gt;与ELU一样具有自归一化特性，与BN不完全兼容。&lt;/li&gt;
&lt;li&gt;CELU的表现依赖于超参数$\alpha$，该参数控制了负区间的输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(11) GELU激活函数&lt;/strong&gt;&lt;/p&gt;
$$
GELU(x) = 0.5x(1+tanh(\sqrt{\frac{2}{\pi}}(x+0.044715x^3)))
$$&lt;img src=&#34;gelu.png&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;当方差为无穷大，均值为0的时候，GeLU就等价于ReLU了。GELU可以当作为RELU的一种平滑策略。GELU是非线性输出，具有一定的连续性。&lt;/li&gt;
&lt;li&gt;负值范围输出非零梯度，防止了梯度消失。&lt;/li&gt;
&lt;li&gt;GELU利用高斯误差函数(误差函数与标准正态分布密切相关)，对输入进行概率化变换，这种特性有助于神经网络学习更加复杂和非线性的模式。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GELU的优势主要体现在深度神经网络中，对于较浅的网络，ReLU等激活函数可能已经足够好。使用GELU可能无法提供显著的性能提升，反而由于计算开销的增加，可能会降低网络的训练和推理效率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(12) Swish激活函数&lt;/strong&gt;&lt;/p&gt;
$$
f(x) = x·\sigma(\beta x)
$$$$
f&#39;(x) = xf(\beta x)(1-f(\beta x)) + f(\beta x)
$$&lt;img src=&#34;swish.png&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;和ReLU一样，没有上边界，因此不会出现梯度饱和的现象(避免过拟合)。&lt;/li&gt;
&lt;li&gt;有下边界，可以产生更强的正则化效果(x左半轴慢慢趋近0)。&lt;/li&gt;
&lt;li&gt;连续可导，且在整个输入空间中具有平滑的过渡，使Swish能够更好地保持梯度信息，并且在训练时能更好地传播梯度，帮助深度神经网络更有效地学习复杂的特征。&lt;/li&gt;
&lt;li&gt;自门控特性，即函数的输出由输入的本身和经过Sigmoid变换后的输入共同决定。这使得Swish在网络的每一层都能根据输入的特征来动态地调整激活函数的“门控”程度，从而更适应输入的不同情况。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Swish的优势主要体现在深度神经网络中，特别是在处理更复杂的特征时。在一些浅层网络上，Swish可能不会比其他激活函数(如 ReLU或Sigmoid)带来显著的性能提升。&lt;/li&gt;
&lt;li&gt;由于其涉及到Sigmoid函数的计算，Swish的计算效率不如ReLU，在推理时的延迟可能较高。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(13) SwiGLU激活函数&lt;/strong&gt;&lt;/p&gt;
$$
SwiGLU(x) = Swish(x)·\sigma (Wx+b)
$$&lt;p&gt;
其中，$Swish(x)=x·\sigma (x)$，即Swish激活函数；$\sigma(Wx+b)$是Sigmoid门控函数，控制激活值的开关。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;结合了Swish和GLU的特性，具有非线性表示能力 和门控机制。&lt;/li&gt;
&lt;li&gt;在深度神经网络中，能够加速训练收敛，并提高梯度流动和信息传播效率。&lt;/li&gt;
&lt;li&gt;通过门控机制避免了梯度爆炸和梯度消失问题，增强了训练稳定性。&lt;/li&gt;
&lt;li&gt;具有平滑的激活函数，能学习更加复杂的模式。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算开销相对较大，计算复杂性较高。&lt;/li&gt;
&lt;li&gt;不适合浅层网络。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;梯度消失和梯度爆炸&#34;&gt;梯度消失和梯度爆炸
&lt;/h2&gt;&lt;p&gt;在前面激活函数的介绍中我们多次提到了梯度消失和梯度爆炸的概念，下面再对其概念和解决方法做一下总结。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;梯度消失&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当反向传播层数逐渐增多，由于每一层都对前一层梯度乘以了一个小数，因此随着反向传播，梯度逐层减小，导致靠近输入层的权重更新极慢甚至停止更新，网络难以学习到有效的特征。&lt;/p&gt;
&lt;p&gt;解决方案：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择合适的激活函数，如ReLU(大于0时导数为1)、Leaky ReLU(在ReLU能力上避免了神经元死亡问题)、ELU(保持ReLu有时的同时解决Dead ReLU问题)等。&lt;/li&gt;
&lt;li&gt;采用ResNet残差连接，通过“跳跃连接”直接把输入传递给后续层，使得梯度可以绕过某些层，解决梯度消失问题，使得网络可以训练非常深。&lt;/li&gt;
&lt;li&gt;适当的权重初始化：Xavier初始化/He初始化，防止梯度在传播过程中过大或过小。&lt;/li&gt;
&lt;li&gt;使用Batch Normalization/Layer Normalization。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;梯度爆炸&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;随着误差反向传播，梯度值逐层增大，导致靠近输入层的权重更新过大，从而使模型训练变得不稳定，甚至梯度数值溢出(NaN)。其原因是&lt;strong&gt;初始权重过大&lt;/strong&gt;，导致梯度不断放大&lt;/p&gt;
&lt;p&gt;解决方案：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;梯度裁剪，即设定一个剪切阈值，当梯度过大时，将其限制在一定范围。&lt;/li&gt;
&lt;li&gt;采用L2正则化约束权重大小，防止梯度过大。&lt;/li&gt;
&lt;li&gt;适当的权重初始化：Xavier初始化/He初始化，防止梯度在传播过程中过大或过小。&lt;/li&gt;
&lt;li&gt;使用Batch Normalization/Layer Normalization。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;权重初始化的具体方法、Normalization、正则化、优化器我们在下章再详细介绍。&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference
&lt;/h2&gt;&lt;p&gt;[1] &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/illikang/article/details/82019945?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522b8ddfd309f99fabf168aaa1b10b4063a%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;amp;request_id=b8ddfd309f99fabf168aaa1b10b4063a&amp;amp;biz_id=0&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-82019945-null-null.142%5ev102%5epc_search_result_base9&amp;amp;utm_term=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;amp;spm=1018.2226.3001.4187&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/illikang/article/details/82019945?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522b8ddfd309f99fabf168aaa1b10b4063a%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=b8ddfd309f99fabf168aaa1b10b4063a&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-82019945-null-null.142^v102^pc_search_result_base9&amp;utm_term=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;spm=1018.2226.3001.4187&lt;/a&gt;&lt;br&gt;
[2] &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/seasonsyy/article/details/136124829?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522e47868d831ef55e9c4b37db9be731232%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;amp;request_id=e47868d831ef55e9c4b37db9be731232&amp;amp;biz_id=0&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-136124829-null-null.142%5ev102%5epc_search_result_base9&amp;amp;utm_term=%E6%84%9F%E7%9F%A5%E6%9C%BA%E8%AE%AD%E7%BB%83&amp;amp;spm=1018.2226.3001.4187&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/seasonsyy/article/details/136124829?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522e47868d831ef55e9c4b37db9be731232%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=e47868d831ef55e9c4b37db9be731232&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-136124829-null-null.142^v102^pc_search_result_base9&amp;utm_term=%E6%84%9F%E7%9F%A5%E6%9C%BA%E8%AE%AD%E7%BB%83&amp;spm=1018.2226.3001.4187&lt;/a&gt;&lt;br&gt;
[3] &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/illikang/article/details/82019945?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522b8ddfd309f99fabf168aaa1b10b4063a%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;amp;request_id=b8ddfd309f99fabf168aaa1b10b4063a&amp;amp;biz_id=0&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-82019945-null-null.142%5Ev102%5Epc_search_result_base9&amp;amp;utm_term=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;amp;spm=1018.2226.3001.4187https://blog.csdn.net/illikang/article/details/82019945?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522b8ddfd309f99fabf168aaa1b10b4063a%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;amp;request_id=b8ddfd309f99fabf168aaa1b10b4063a&amp;amp;biz_id=0&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-82019945-null-null.142%5Ev102%5Epc_search_result_base9&amp;amp;utm_term=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;amp;spm=1018.2226.3001.4187&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/illikang/article/details/82019945?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522b8ddfd309f99fabf168aaa1b10b4063a%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=b8ddfd309f99fabf168aaa1b10b4063a&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-82019945-null-null.142%5Ev102%5Epc_search_result_base9&amp;utm_term=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;spm=1018.2226.3001.4187https://blog.csdn.net/illikang/article/details/82019945?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522b8ddfd309f99fabf168aaa1b10b4063a%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=b8ddfd309f99fabf168aaa1b10b4063a&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-82019945-null-null.142%5Ev102%5Epc_search_result_base9&amp;utm_term=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;spm=1018.2226.3001.4187&lt;/a&gt;&lt;br&gt;
[4] &lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/369328305&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://zhuanlan.zhihu.com/p/369328305&lt;/a&gt;&lt;br&gt;
[5] &lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/642537175&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://zhuanlan.zhihu.com/p/642537175&lt;/a&gt;&lt;br&gt;
[6] &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/caip12999203000/article/details/127067360?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522aa20995c1e0fe5de0e2fb06bae4373e4%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;amp;request_id=aa20995c1e0fe5de0e2fb06bae4373e4&amp;amp;biz_id=0&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-127067360-null-null.142%5ev102%5epc_search_result_base9&amp;amp;utm_term=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&amp;amp;spm=1018.2226.3001.4187&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/caip12999203000/article/details/127067360?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522aa20995c1e0fe5de0e2fb06bae4373e4%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=aa20995c1e0fe5de0e2fb06bae4373e4&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-127067360-null-null.142^v102^pc_search_result_base9&amp;utm_term=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&amp;spm=1018.2226.3001.4187&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>多模态RAG | ColPali: Efficient Document Retrieval with Vision Language Models</title>
        <link>https://JLING-L.github.io/p/%E5%A4%9A%E6%A8%A1%E6%80%81rag-colpali-efficient-document-retrieval-with-vision-language-models/</link>
        <pubDate>Sun, 30 Mar 2025 13:10:43 +0800</pubDate>
        
        <guid>https://JLING-L.github.io/p/%E5%A4%9A%E6%A8%A1%E6%80%81rag-colpali-efficient-document-retrieval-with-vision-language-models/</guid>
        <description>&lt;img src="https://JLING-L.github.io/p/%E5%A4%9A%E6%A8%A1%E6%80%81rag-colpali-efficient-document-retrieval-with-vision-language-models/colpali.png" alt="Featured image of post 多模态RAG | ColPali: Efficient Document Retrieval with Vision Language Models" /&gt;</description>
        </item>
        <item>
        <title>通用信息抽取3 | InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction</title>
        <link>https://JLING-L.github.io/p/%E9%80%9A%E7%94%A8%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%963-instructuie-multi-task-instruction-tuning-for-unified-information-extraction/</link>
        <pubDate>Mon, 04 Nov 2024 13:31:44 +0800</pubDate>
        
        <guid>https://JLING-L.github.io/p/%E9%80%9A%E7%94%A8%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%963-instructuie-multi-task-instruction-tuning-for-unified-information-extraction/</guid>
        <description>&lt;img src="https://JLING-L.github.io/p/%E9%80%9A%E7%94%A8%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%963-instructuie-multi-task-instruction-tuning-for-unified-information-extraction/framework.jpg" alt="Featured image of post 通用信息抽取3 | InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction" /&gt;&lt;img src=&#34;authors.jpg&#34; style=&#34;zoom:70%&#34; /&gt;
&lt;p&gt;论文：https://arxiv.org/abs/2304.08085&lt;br&gt;
代码：https://github.com/BeyonderXX/InstructUIE&lt;/p&gt;
&lt;img src=&#34;introduction.jpg&#34; style=&#34;zoom:60%&#34; /&gt;
&lt;p&gt;之前的笔记中我们介绍了UIE(通用信息抽取1)和USM(通用信息抽取2)。我们先回顾一下这两篇工作：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;UIE&lt;br&gt;
UIE通过设计的结构提取语言SEL对异构IE结构进行统一建模，使用结构模式指令器SSI为不同IE任务生成目标结构，通过在多个数据集上预训练模型捕获通用IE能力。&lt;br&gt;
UIE将IE任务统一为seq2seq生成任务，输入为文本形式，输出为对应的结构化信息。&lt;br&gt;
局限性：由于不同任务的输出模式可能不同，UIE需要针对不同的下游任务进行单独的微调，以生成特定格式的输出。&lt;/li&gt;
&lt;li&gt;USM&lt;br&gt;
USM将IE任务解耦为结构化和概念化两个基本任务。通过TTL操作结构化信息，获取基本信息单元。通过LTL、TLL操作概念化信息，在label和基本信息单元间建立链接。&lt;br&gt;
USM基于语义匹配机制，通过比对文本span与语义标签的相似性来完成IE任务。&lt;br&gt;
局限性：将IE转化为语义匹配任务，难以与生成式语言模型集成；需要对每个单词进行语义匹配，导致训练和推理时间显著增加。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;LLM通过多任务训练和统一编码展现出强大的泛化能力，而有研究表明，LLM在IE任务上仍有显著的性能差距。&lt;/p&gt;
&lt;p&gt;那么，是否有办法激发LLM的潜在知识来完成IE任务？&lt;/p&gt;
&lt;h2 id=&#34;instructuie&#34;&gt;InstructUIE
&lt;/h2&gt;&lt;h3 id=&#34;task-schema&#34;&gt;Task Schema
&lt;/h3&gt;&lt;p&gt;为了更好地转移和利用在预训练语言模型中学习到的知识，将IE任务重新表述为seq2seq形式，并微调LLM。如图所示，每个任务实例都用四个属性格式化：&lt;br&gt;
&lt;img src=&#34;example1.jpg&#34; style=&#34;zoom:50%&#34; /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;任务指令(Task Instruction)：提供详细的指导，说明如何从输入文本中提取相关信息，并生成所需的输出结构。具体内容包括要提取的信息类型、输出结构的格式以及提取过程中需要遵循的额外约束或规则。&lt;/li&gt;
&lt;li&gt;选项(Options)：定义任务的输出标签约束，表示模型可以为给定输入生成的可能输出集。例如，在命名实体识别(NER)中，选项可能包括&amp;quot;person&amp;quot;、&amp;ldquo;organization&amp;rdquo;、&amp;ldquo;location&amp;quot;等实体标签；在关系提取(RE)中，选项可能代表可以提取的关系类型，如&amp;quot;works for&amp;rdquo;、&amp;ldquo;born in&amp;quot;等；在事件提取(EE)中，选项可能对应不同事件类型的标签，如&amp;quot;beginning&amp;rdquo;、&amp;ldquo;end&amp;quot;等。&lt;/li&gt;
&lt;li&gt;文本(Text)：任务实例的输入句子，与任务指令和选项一起输入预训练的语言模型，使模型能够为给定任务生成所需的输出序列。&lt;/li&gt;
&lt;li&gt;输出(Output)：将样本的原始标签转换为句子格式。对于NER，输出格式为&amp;quot;entity tag: entity span&amp;rdquo;；对于RE，输出格式为&amp;quot;relationship: head entity, tail entity&amp;quot;； 对于EE，输出格式为&amp;quot;event tag: trigger word, argument tag: argument span&amp;quot;； 如果输入中没有与提供选项匹配的结构信息，则输出&amp;quot;None&amp;quot;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;auxiliary-tasks&#34;&gt;Auxiliary Tasks
&lt;/h3&gt;&lt;p&gt;辅助任务，与主任务一起训练。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NER：引入了span extraction task和entity typing task。span extraction task旨在从输入句子中提取实体span，而entity typing task旨在识别实体的类型。&lt;/li&gt;
&lt;li&gt;RE：引入了entity pair extraction task和relation classification task。entity pair extraction task旨在提取关系中涉及的实体对，而relation classification task旨在对实体对之间的关系类型进行分类。&lt;/li&gt;
&lt;li&gt;EE：引入了trigger extraction task和argument extraction task。trigger extraction task旨在提取触发事件的触发词，而argument extraction task旨在提取相关的参数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ie-instructions&#34;&gt;IE INSTRUCTIONS
&lt;/h2&gt;&lt;p&gt;IE INSTRUCTIONS收集了32个公开可用的数据集，涵盖三种类型的IE任务：NER、RE和EE。除了新闻和维基数据等通用领域的来源之外，还包括来自科学、医疗保健、社交媒体和交通等各个领域的语料库，以确保数据集的多样性。&lt;br&gt;
数据处理步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;统一不同数据集中语义相同但名称不同的标签的名称。&lt;/li&gt;
&lt;li&gt;将带有下划线、缩写或特殊格式的标签转换为自然语言格式。例如，将标签&amp;quot;people person place of birth&amp;quot;重命名为&amp;quot;place of birth&amp;quot;。&lt;/li&gt;
&lt;li&gt;将所有数据集转换为文本到文本格式，确保所有任务中输入输出对的一致表示。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;数据集构成如图所示：&lt;br&gt;
&lt;img src=&#34;dataset.jpg&#34; style=&#34;zoom:50%&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments
&lt;/h2&gt;&lt;p&gt;backbone：FlanT5-11B&lt;/p&gt;
&lt;h3 id=&#34;supervised-experiments&#34;&gt;Supervised Experiments
&lt;/h3&gt;&lt;p&gt;Supervised学习设置：在训练期间为所有任务提供指令，模型在每个任务的标注数据上进行微调，使其能够学习特定任务的特征。为了平衡数据集，作者采用了抽样策略。即为每个数据集采样10,000个示例，少于10,000个样本的数据集使用所有示例。&lt;br&gt;
&lt;img src=&#34;supervised_experiment1.jpg&#34; style=&#34;zoom:65%&#34; /&gt;&lt;br&gt;
&lt;img src=&#34;supervised_experiment2.jpg&#34; style=&#34;zoom:53%&#34; /&gt;&lt;br&gt;
&lt;img src=&#34;supervised_experiment3.jpg&#34; style=&#34;zoom:57%&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;zero-shot-experiments&#34;&gt;Zero-shot Experiments
&lt;/h3&gt;&lt;p&gt;Zero-shot学习设置：在训练期间仅为部分任务提供指令，模型在未见过的任务上进行评估，无需额外的微调。作者在18个NER数据集和6个RE数据集上训练了模型，并在7个NER数据集和2个RE数据集上进行测试(训练阶段删除了zero-shot实验需要测试的数据集)。&lt;br&gt;
&lt;img src=&#34;zero_shot_experiment1.jpg&#34; style=&#34;zoom:60%&#34; /&gt;&lt;br&gt;
&lt;img src=&#34;zero_shot_experiment2.jpg&#34; style=&#34;zoom:60%&#34; /&gt;&lt;br&gt;
&lt;img src=&#34;zero_shot_experiment3.jpg&#34; style=&#34;zoom:60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ps：详细的实验结果可以查看原文。&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;提出了UIE的一个End-to-End框架——InstructUIE，它利用自然语言指令来指导IE任务。&lt;/li&gt;
&lt;li&gt;引入新的基准数据集IE INSTRUCTIONS。该基准由32个不同的信息提取数据集组成，这些数据集已统一为文本到文本格式，允许对各种IE任务进行一致和标准化的评估。&lt;/li&gt;
&lt;li&gt;实验结果表明，InstructUIE在监督和zero-shot下实现了SOTA，使用单个多任务模型解决了多个任务。&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>通用信息抽取2 | Universal Information Extraction as Unified Semantic Matching</title>
        <link>https://JLING-L.github.io/p/%E9%80%9A%E7%94%A8%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%962-universal-information-extraction-as-unified-semantic-matching/</link>
        <pubDate>Sun, 03 Nov 2024 09:27:34 +0800</pubDate>
        
        <guid>https://JLING-L.github.io/p/%E9%80%9A%E7%94%A8%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%962-universal-information-extraction-as-unified-semantic-matching/</guid>
        <description>&lt;img src="https://JLING-L.github.io/p/%E9%80%9A%E7%94%A8%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%962-universal-information-extraction-as-unified-semantic-matching/cover.jpg" alt="Featured image of post 通用信息抽取2 | Universal Information Extraction as Unified Semantic Matching" /&gt;&lt;img src=&#34;authors.jpg&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;论文：https://arxiv.org/abs/2301.03282&lt;br&gt;
Accepted by AAAI2023&lt;/p&gt;
&lt;p&gt;上一篇我们介绍了UIE，它采用了文本-结构、seq2seq的生成方法。这一方法具有&amp;quot;黑盒&amp;quot;特性，信息片段(如实体、关系、关键词等)和模式(如输出结构)之间的关联是隐式的，这可能导致以下问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;由于模型在生成过程中不显式地展示哪些信息片段与模式相关联，因此难以判断模型学到了哪些知识。无法判断它是否真正理解了实体关系，还是仅仅是在模拟训练数据的表面模式。&lt;/li&gt;
&lt;li&gt;由于信息片段和生成模式的关联是隐式的，在迁移到新的任务或模式时，模型是否能够正确应用已有知识变得难以预测。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note：输入的信息被编码器编码成一组上下文向量，这些向量包含了输入的所有信息。但这些向量本身是高度抽象的，并没有显式地表示出输入中的特定信息片段(如重要的实体或关系)。而输出的每一步依赖于隐含的上下文向量和之前的输出，这种生成方式也没有显式地表现模型是如何将某些输入信息片段映射到具体的输出部分的。由于这些关联是隐含的，很难解释模型在生成过程中是如何决策的。例如，模型可能在输出中提到了某个关键实体，但难以追踪它是根据哪个输入信息片段得出的。&lt;/p&gt;
&lt;p&gt;因此，作者认为，为了确保有效、稳健和可解释的迁移能力，显式建模和学习可迁移的知识是必要的。&lt;/p&gt;
&lt;h2 id=&#34;unified-semantic-matching-via-directed-token-linking&#34;&gt;Unified Semantic Matching via Directed Token Linking
&lt;/h2&gt;&lt;h3 id=&#34;schema-text-joint-embedding&#34;&gt;Schema-Text Joint Embedding
&lt;/h3&gt;&lt;p&gt;为了让模型理解标签模式(schema)和文本之间的关联(同时捕捉标签和文本之间的语义关系)，USM对提取模式(schema)和文本token进行共同编码，从而生成它们的联合上下文嵌入：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提取模式词序列化：USM将提取模式(schema)表示为一个词序列。例如，如果提取模式是&amp;quot;person&amp;quot;和&amp;quot;location&amp;quot;这样的标签，USM会将这些标签转换为词序列形式，即{$l_1,l_2,&amp;hellip;,l_{|l|}$}。(类似文本token)&lt;/li&gt;
&lt;li&gt;拼接schema和文本：USM将词序列化后的schema sequence与文本token{$t_1,t_2,&amp;hellip;,t_{|t|}$}进行拼接，一起作为模型的输入。&lt;/li&gt;
&lt;li&gt;Transformer编码器：拼接后的词序列和文本通过一个Transformer编码器进行处理，计算出联合的上下文嵌入H，表示为[$h_1,h_2,&amp;hellip;,h_{|l|+|t|}$]。&lt;/li&gt;
&lt;li&gt;Mask矩阵：M是一个Mask矩阵，用来决定哪些标签或文本token可以互相&amp;quot;看到&amp;quot;(self-attention)，从而捕捉它们之间的交互关系。&lt;/li&gt;
&lt;/ol&gt;
$$
H = Encoder(l_1,l_2,...,l_{|l|},t_1,t_2,...,t_{|t|},M)
$$&lt;h3 id=&#34;token-linking-operations&#34;&gt;Token Linking Operations
&lt;/h3&gt;&lt;p&gt;要实现UIE，还是要对不同IE任务进行统一建模。&lt;br&gt;
类似UIE的定位(Spotting)和关联(Associating)，作者将IE任务解耦为两个操作：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;结构化(Structuring)：从文本中抽取出与标签无关的基本子结构。例如，实体抽取&amp;quot;Monet&amp;quot;；事件触发词&amp;quot;born in&amp;quot;；关系实体对(&amp;ldquo;Monet&amp;rdquo;，&amp;ldquo;Paris&amp;rdquo;)；事件论元(&amp;ldquo;born in&amp;rdquo;，&amp;ldquo;Paris&amp;rdquo;)。&lt;/li&gt;
&lt;li&gt;概念化(Conceptualizing)：在概念化阶段将这些具体子结构与语义标签相连接。例如：&amp;ldquo;Monet&amp;quot;可以标记为&amp;quot;person&amp;rdquo;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中，结构化通过Token-Token Linking (TTL)操作实现，概念化通过Label-Token Linking (LTL)和Token-Label Linking (TLL)操作实现。&lt;/p&gt;
&lt;h4 id=&#34;token-token-linking-for-structuring-ttl&#34;&gt;Token-Token Linking for Structuring (TTL)
&lt;/h4&gt;&lt;p&gt;前面我们提到，结构化的目标是要提取出文本中的基本信息单元(如实体、关系、触发词等)。这些基本信息单元是一组连续的token序列，我们需要知道单个信息单元是从哪里开始、又从哪里结束。&lt;br&gt;
而部分IE任务所需的结构可能包括不止一个信息单元，例如{&amp;ldquo;Monet&amp;rdquo;, birth place, &amp;ldquo;Paris&amp;rdquo;}就是一个三元组的形式，此时，我们则需要找到Subject-Object对。&lt;br&gt;
作者设计了TTL操作，从两个方面对输入文本中所有有效的子结构进行构造：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Utterance(文本)：指输入文本中的连续token序列，例如实体(如&amp;quot;Monet&amp;quot;)或事件触发词(如&amp;quot;born in&amp;quot;)。&lt;br&gt;
&lt;img src=&#34;TTL_example1.jpg&#34; style=&#34;zoom:40%&#34; /&gt;&lt;br&gt;
如图所示，其实就是将头和尾链接(H2T)。例如，为了提取span&amp;quot;Monet&amp;quot;和&amp;quot;born in&amp;quot;作为有效的子结构，USM利用H2T将&amp;quot;Monet&amp;quot;链接到自身来提取&amp;quot;Monet&amp;quot;，并将&amp;quot;born&amp;quot;链接到&amp;quot;in&amp;quot;以提取&amp;quot;born in&amp;quot;。&lt;/li&gt;
&lt;li&gt;Association Pair(文本对)：从文本中提取的基本关联对单元，例如关系的Subject-Object对(如&amp;quot;Monet&amp;quot;和&amp;quot;Paris&amp;quot;)或事件触发词-参数对(如&amp;quot;born in&amp;quot;和&amp;quot;Paris&amp;quot;)。&lt;br&gt;
&lt;img src=&#34;TTL_example2.jpg&#34; style=&#34;zoom:40%&#34; /&gt;&lt;br&gt;
如图所示，链接关联对是将头与头链接(H2H)、尾与尾链接(T2T)。例如，要提取Subject-Object对&amp;quot;Monet&amp;quot;和&amp;quot;Paris&amp;quot;，USM通过H2H将&amp;quot;Monet&amp;quot;和&amp;quot;Paris&amp;quot;链接起来，同时也通过T2T链接这两个token。&lt;/li&gt;
&lt;/ol&gt;
$$
s_{TTL}(t_i,t_j) = FFNN_{TTL}^l(h_t^i)^TR_{j-i}FFNN_{TTL}^r(h_t^j)
$$&lt;p&gt;
其中，FFNN为前馈神经网络层，用于处理token的嵌入信息。$R_{j-i}$是一个旋转位置嵌入(用于编码token之间的相对位置关系)。通过这种方式，模型可以捕捉到token之间的相对位置关系。&lt;/p&gt;
&lt;h4 id=&#34;label-token-linking-for-utterance-conceptualizing-ltl&#34;&gt;Label-Token Linking for Utterance Conceptualizing (LTL)
&lt;/h4&gt;&lt;p&gt;结构化是从Utterance和Association Pair两个方面来进行处理的，概念化也是如此。首先考虑Utterance的概念化，包括两种类型：&lt;br&gt;
&lt;img src=&#34;LTL_example.jpg&#34; style=&#34;zoom:40%&#34; /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Type of Mention：将标签类型直接分配给span。例如，将实体&amp;quot;Monet&amp;quot;标记为类型&amp;quot;person&amp;quot;，&amp;ldquo;France&amp;quot;标记为类型&amp;quot;country&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;Predicate of Object：将关系标签分配给Association Pair中的Object。例如，&amp;ldquo;Paris&amp;quot;的关系类型&amp;quot;birth place&amp;rdquo;，事件参数类型&amp;quot;capital&amp;quot;。
用label-to-head(L2H)和label-to-tail(L2T)链接操作来为每个子结构分配标签(其实也是头对头、尾对尾，只是不是将信息单元进行链接了，而是将标签与信息单元进行链接)。链接分数的计算公式如下：
$$
s_{LTL}(l_i,t_j) = FFNN_{LTL}^{label}(h_i^l)^TR_{j-i}FFNN_{LTL}^{Text}(h_j^t)
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;token-label-linking-for-pairing-conceptualizing-tll&#34;&gt;Token-Label Linking for Pairing Conceptualizing (TLL)
&lt;/h4&gt;$$
s_{TTL}(t_i,l_j) = FFNN_{TLL}^{text}(h_i^l)^TR_{j-i}FFNN_{TLL}^{label}(h_j^t)
$$&lt;h3 id=&#34;schema-text-joint-embedding-1&#34;&gt;Schema-Text Joint Embedding
&lt;/h3&gt;&lt;p&gt;通过schema-constraint解码，如图所示：&lt;br&gt;
&lt;img src=&#34;framework.jpg&#34; style=&#34;zoom:60%&#34; /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过TTL操作提取的内容：例如，{&amp;ldquo;Monet&amp;rdquo;, &amp;ldquo;Paris&amp;rdquo;, &amp;ldquo;France&amp;rdquo;, (&amp;ldquo;Monet&amp;rdquo;, &amp;ldquo;Pairs&amp;rdquo;), (&amp;ldquo;France&amp;rdquo;, &amp;ldquo;Pairs&amp;rdquo;)}，抽取得到&amp;quot;Monet&amp;quot;和&amp;quot;Paris&amp;quot;，以及Subject-Object对(&amp;ldquo;Monet&amp;rdquo;, &amp;ldquo;Paris&amp;rdquo;)和(&amp;ldquo;France&amp;rdquo;, &amp;ldquo;Paris&amp;rdquo;)。&lt;/li&gt;
&lt;li&gt;通过LTL操作提取的内容：例如，{(person, &amp;ldquo;Monet&amp;rdquo;), (country, &amp;ldquo;France&amp;rdquo;), (birth place, &amp;ldquo;Paris&amp;rdquo;), (capital, &amp;ldquo;Paris&amp;rdquo;)}，每个label(如person、country)都与相应的Object(如&amp;quot;Monet&amp;quot;、&amp;ldquo;France&amp;rdquo;)相关联。&lt;/li&gt;
&lt;li&gt;通过TLL操作提取的内容：例如，(&amp;ldquo;Monet&amp;rdquo;, birth place), (&amp;ldquo;France&amp;rdquo;, capital)，将Subject与其相应的关系label(如birth place、capital)联系起来。&lt;/li&gt;
&lt;li&gt;基于获得的label-token间的链接(例如&amp;quot;Monet&amp;quot;与&amp;quot;birth place&amp;quot;以及&amp;quot;France&amp;quot;与&amp;quot;capital&amp;quot;)，USM能够一致地生成完整的结构，例如：(&amp;ldquo;Monet&amp;rdquo;, birth place, &amp;ldquo;Paris&amp;rdquo;)和(&amp;ldquo;France&amp;rdquo;, capital, &amp;ldquo;Paris&amp;rdquo;)。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些解码步骤之间是独立的，这意味着提取操作是完全非自回归的(non-autoregressive)，这使得解码过程可以并行进行。&lt;/p&gt;
&lt;h2 id=&#34;learning-from-heterogeneous-supervision&#34;&gt;Learning from Heterogeneous Supervision
&lt;/h2&gt;&lt;p&gt;USM使用了标签的词序列化表示(verbalized label representation)和统一的token链接机制，不论资源的来源或类型如何，USM将它们统一表示为&amp;lt;text, token pairs&amp;gt;进行预训练。&lt;/p&gt;
&lt;h3 id=&#34;pre-training&#34;&gt;Pre-training
&lt;/h3&gt;&lt;p&gt;异构监督资源(来源不同、类型多样的训练数据)包括任务注释信号(例如，IE数据集)、远程信号(例如，远程监督数据集)和间接信号(例如，QA数据集)三种不同类型：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$D_{task}$(任务标注数据集)&lt;br&gt;
其中每个实例都有一个用于信息提取的标准注释。本文使用在信息提取领域广泛使用的Ontonotes作为标准注释，它包含18种实体类型。
任务标注数据集用于帮助模型学习任务特定的结构化和概念化能力。即通过这些标注数据，模型可以学习如何准确地从文本中提取特定类型的实体和关系。&lt;/li&gt;
&lt;li&gt;$D_{distant}$(远程监督数据集)&lt;br&gt;
远程监督数据集是通过将文本与知识库对齐来生成的标注数据。本文使用NYT和Rebel作为远程监督数据集，这些数据集分别是通过将文本与Freebase和Wikidata对齐而获得。由于Rebel数据集的标签模式过长，不适合直接与输入文本拼接并输入到预训练的Transformer编码器中。因此，本文采样了负标签(与输入文本的实际内容没有对应关系的标签)模式构建meta schema，简化预训练时的标签模式。
远程监督数据为模型提供大规模的训练数据，帮助模型学习广泛的结构和概念化能力。&lt;/li&gt;
&lt;li&gt;$D_{indirect}$(间接监督数据集)&lt;br&gt;
间接监督数据集来自与信息抽取相关的其他NLP任务，主要是阅读理解数据集。本文使用MRQA中的阅读理解数据集，如HotpotQA、Natural Questions、NewsQA、SQuAD、TriviaQA等。
间接监督数据集通过多样化的问题表达，提供了比$D_{task}$和$D_{distant}$更多样化的标签语义信息，有助于模型学习更丰富的概念化能力(增强泛化能力)。对于每个(question，context，answer)实例，问题作为label schema，context作为输入文本，answer作为mention。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;learning-function&#34;&gt;Learning function
&lt;/h3&gt;$$
L = \sum_{m \in \mathcal{M}} log \left(1+\sum_{(i, j) \in m^{+}} e^{-s_{m}(i, j)}\right) +\log \left(1+\sum_{(i, j) \in m^{-}} e^{s_{m}(i, j)}\right)
$$&lt;p&gt;
损失函数由两部分组成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;针对已链接对$m^+$的损失，利用链接评分$s_m (i,j)$来计算概率，增加与正样本的联系。&lt;/li&gt;
&lt;li&gt;针对未链接对$m^-$的损失，同样基于链接评分，但通过对未链接的对进行惩罚，降低其影响。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments
&lt;/h2&gt;&lt;p&gt;对比USM和其他SOTA模型在4个任务的13个数据集上的性能。AVE-unify表示非重叠数据集的平均性能，AVE-total表示所有数据集的平均性能。USM实现了SOTA，在不使用预训练模型的情况下，用RoBERTa初始化的USM框架也表现出了较好的效果。&lt;br&gt;
&lt;img src=&#34;experiment1.jpg&#34; style=&#34;zoom:45%&#34; /&gt;&lt;br&gt;
在不同领域的9个数据集上进行zero-shot实验。即使从实体类型有限的$D_{task}$中学习，USM在电影、文学和音乐领域也表现出良好的迁移性能。$D_{distant}$和$D_{indirect}$在预训练过程中有重要作用。&lt;br&gt;
&lt;img src=&#34;experiment2.jpg&#34; style=&#34;zoom:45%&#34; /&gt;&lt;br&gt;
USM(356M)以较小的模型大小优于强zero-shot基线GPT-3(175B)和DEEPSTRUCTURE(10B)。&lt;br&gt;
&lt;img src=&#34;experiment3.jpg&#34; style=&#34;zoom:45%&#34; /&gt;&lt;br&gt;
(详细的实验结果和分析可以查看原文)&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;提出了一个统一的语义匹配框架——USM。&lt;/li&gt;
&lt;li&gt;UIE方法未能显式地展示信息片段与模式的关联、知识迁移效果难以预测 ——&amp;gt; USM通过结构化和概念化显式地建立了信息片段与标签之间的关联&lt;br&gt;
(例如，当迁移到新任务时，USM不需要重新学习基本的信息抽取能力，而是只需要在新领域中重新定义或调整概念化标签。这种显式关联使得知识的迁移更加系统化，减少了模型在新任务中的不确定性)&lt;/li&gt;
&lt;li&gt;USM在监督实验下达到了最先进的性能，在zero/few shot设置下表现出很强的泛化能力。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;p&gt;[1] &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/qq_27590277/article/details/128699655?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522A7AAA045-C8D2-470D-8CB4-6FB0D7220C7D%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;amp;request_id=A7AAA045-C8D2-470D-8CB4-6FB0D7220C7D&amp;amp;biz_id=0&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-2-128699655-null-null.142%5ev100%5epc_search_result_base1&amp;amp;utm_term=Universal%20Information%20Extraction%20as%20Unified%20Semantic%20Matching&amp;amp;spm=1018.2226.3001.4187&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/qq_27590277/article/details/128699655?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522A7AAA045-C8D2-470D-8CB4-6FB0D7220C7D%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=A7AAA045-C8D2-470D-8CB4-6FB0D7220C7D&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-2-128699655-null-null.142^v100^pc_search_result_base1&amp;utm_term=Universal%20Information%20Extraction%20as%20Unified%20Semantic%20Matching&amp;spm=1018.2226.3001.4187&lt;/a&gt;&lt;br&gt;
[2] &lt;a class=&#34;link&#34; href=&#34;https://adaning.github.io/posts/11838.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://adaning.github.io/posts/11838.html&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>通用信息抽取1 | Unified Structure Generation for Universal Information Extraction</title>
        <link>https://JLING-L.github.io/p/%E9%80%9A%E7%94%A8%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%961-unified-structure-generation-for-universal-information-extraction/</link>
        <pubDate>Wed, 30 Oct 2024 16:37:14 +0800</pubDate>
        
        <guid>https://JLING-L.github.io/p/%E9%80%9A%E7%94%A8%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%961-unified-structure-generation-for-universal-information-extraction/</guid>
        <description>&lt;img src="https://JLING-L.github.io/p/%E9%80%9A%E7%94%A8%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%961-unified-structure-generation-for-universal-information-extraction/uie_framwork.jpg" alt="Featured image of post 通用信息抽取1 | Unified Structure Generation for Universal Information Extraction" /&gt;&lt;img src=&#34;authors.jpg&#34; style=&#34;zoom:43%&#34; /&gt;
&lt;p&gt;论文：https://arxiv.org/abs/2203.12277&lt;br&gt;
代码：https://github.com/universal-ie/UIE&lt;br&gt;
Accepted to the main conference of ACL2022&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;什么是信息抽取？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们首先简单介绍一下信息抽取，有了解的可以直接跳过。&lt;br&gt;
信息抽取(Information Extraction, IE)是自然语言处理(NLP)领域的一个任务，旨在从非结构化的文本数据中自动识别并提取结构化的信息，例如提取文本中的关键实体、关系或事件等。主要关注三个任务：&lt;br&gt;
例：Steve became CEO of Apple in 1997.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;命名实体识别(Named Entity Recognition, NER)&lt;br&gt;
NER任务的目标是识别文本中的实体(如人名、地点、组织等)并分类。对于上面给出的例句，识别实体&amp;quot;Steve&amp;quot;，类型标注为&amp;quot;person&amp;quot;；识别实体&amp;quot;Apple&amp;quot;，类型标注为&amp;quot;organization&amp;quot;；识别实体&amp;quot;1997&amp;quot;，类型标注为&amp;quot;time&amp;quot;。&lt;/li&gt;
&lt;li&gt;关系抽取(Relation Extraction)&lt;br&gt;
RE任务的目标是识别文本中的实体关系，明确它们之间的联系。对于上面给出的例句，头实体&amp;quot;Steve&amp;quot;，尾实体&amp;quot;Apple&amp;quot;，他们之间的关系为&amp;quot;work-for&amp;quot;。&lt;/li&gt;
&lt;li&gt;事件抽取(Event Extraction)&lt;br&gt;
EE任务的目标是识别文本中发生的事件，并提取出事件的参与实体、时间、地点等信息(事件参数)。对于上面给出的例句，识别触发词&amp;quot;became&amp;quot;，类型标注为&amp;quot;start-position&amp;quot;。事件下对应的三个论元&amp;quot;Steve&amp;quot;，&amp;ldquo;Apple&amp;rdquo;，&amp;ldquo;1997&amp;quot;分别扮演&amp;quot;employee&amp;rdquo;，&amp;ldquo;employer&amp;rdquo;，&amp;ldquo;time&amp;quot;的事件角色。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;可以看出，对于不同的IE任务，随着任务目标的变化，需要抽取出的信息的结构也各有不同，例如在NER中要抽取出的是句子中的单词或短语，在RE任务中又需要判断两个实体间的关系。&lt;br&gt;
因此要完成不同的任务，就需要定义不同的抽取模式(即信息抽取时需要遵循的结构化规则或框架)。&lt;br&gt;
但可以观察到，这几个任务之间其实是有共通之处的，比如RE，就可以看作是NER任务的进一步扩展。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;是否有办法对这三个任务进行统一建模？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面我们来看一下UIE这篇文章。&lt;/p&gt;
&lt;h2 id=&#34;unified-structure-generation-for-universal-information-extraction&#34;&gt;Unified Structure Generation for Universal Information Extraction
&lt;/h2&gt;&lt;p&gt;IE受到其不同目标、异构结构和特定需求模式的影响，此前大多数IE方法都是为不同的任务设计不同的模型，这存在几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为大量的IE任务/设置/场景开发专用架构非常复杂(比如单独设计一个模型，专门处理RE任务；单独设计一个模型，专门处理EE任务，听起来就很麻烦)。&lt;/li&gt;
&lt;li&gt;学习孤立的模型严重限制了相关任务和设置之间的知识共享(比如RE任务，NER任务中实体已经抽取好了、类别也标注好了，为什么不能直接用呢)。&lt;/li&gt;
&lt;li&gt;构建专门用于不同IE任务的数据集和知识源既昂贵又耗时(不同任务的数据集的知识是不是可以共享呢)。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;作者认为，所有IE任务都可以建模为文本到结构(text-to-structure)的转换。&lt;/p&gt;
&lt;h3 id=&#34;structured-extraction-language-for-uniform-structure-encoding-sel&#34;&gt;Structured Extraction Language for Uniform Structure Encoding (SEL)
&lt;/h3&gt;&lt;p&gt;首先，要将不同的IE结构编码成统一的表示，这样就可以在同一个文本-结构的生成框架中对各种IE任务进行统一建模。具体而言，作者将这一转换过程拆分为了两个子操作：&lt;br&gt;
例：Steve works for Apple.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定位(Spotting)：在文本中找到特定语义类型的span，将文本中的特定span根据预先定义的语义类型进行标注。例：&amp;ldquo;Steve&amp;quot;被标注为&amp;quot;Person&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;关联(Associating)：对已经定位的span进行关联(例如标记实体对之间的关系)。例：将&amp;quot;Steve&amp;quot;标记为Arg1，&amp;ldquo;Apple&amp;quot;标记为Arg2(类似将&amp;quot;Steve&amp;quot;视为头实体，&amp;ldquo;Apple&amp;quot;视为尾实体)。关联Arg1和Arg2，即判断这两个实体之间的关系，我们将关系标注为&amp;quot;Work-for&amp;rdquo;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;作者使用Spot Name和Asso Name分别表示Spotting和Associating操作。Info Span则表示Spot Name和Asso Name所标记的实际文本信息。&lt;br&gt;
我们来看作者给出的一个具体示例。&lt;br&gt;
&lt;img src=&#34;SEL_example.jpg&#34; style=&#34;zoom:45%&#34; /&gt;&lt;br&gt;
例：对于文本输入&amp;quot;Steve became CEO of Apple in 1997.&amp;quot;，通过SEL生成的结构：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RE(蓝色)：头实体&amp;quot;Steve&amp;rdquo;(Info Span)，类型标注为&amp;quot;person&amp;rdquo;(Spot Name)；尾实体&amp;quot;Apple&amp;quot; (Info Span)，类型标注为&amp;quot;organization&amp;quot;(Spot Name)；关系&amp;quot;work-for&amp;quot;(Asso Name)。&lt;/li&gt;
&lt;li&gt;EE(红色)：触发词&amp;quot;became&amp;quot;(Info Span)，类型标注为&amp;quot;start-position&amp;quot;(Spot Name)。事件下对应的三个论元&amp;quot;Steve&amp;quot; (Info Span)，&amp;ldquo;Apple&amp;rdquo; (Info Span)，&amp;ldquo;1997&amp;rdquo; (Info Span)分别扮演&amp;quot;employee&amp;quot;(Spot Name)，&amp;ldquo;employer&amp;rdquo;(Spot Name)，&amp;ldquo;time&amp;rdquo;(Spot Name)的事件角色。&lt;/li&gt;
&lt;li&gt;NER(黑色)：实体&amp;quot;Steve&amp;quot;(Info Span)，类型标注为&amp;quot;person&amp;quot;(Spot Name)；实体&amp;quot;Apple&amp;quot; (Info Span)，类型标注为&amp;quot;organization&amp;quot;(Spot Name)；实体&amp;quot;1997&amp;quot; (Info Span)，类型标注为&amp;quot;time&amp;quot;(Spot Name)。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这样一来，无论对二元关系，还是N元论元结构，都可以用SEL语法表示。&lt;/p&gt;
&lt;h3 id=&#34;structural-schema-instructor-for-controllable-ie-structure-generation-ssi&#34;&gt;Structural Schema Instructor for Controllable IE Structure Generation (SSI)
&lt;/h3&gt;&lt;p&gt;虽然三个IE任务都可以通过SEL统一表示结构，但模型还不知道什么时候该做什么。&lt;br&gt;
作者设计了特殊符号[spot], [asso]和[text]，分别添加到每个Spot Name、Asso Name和输入文本序列之前，来提示模型要提取哪些信息。&lt;br&gt;
&lt;img src=&#34;uie_framwork.jpg&#34; style=&#34;zoom:42%&#34; /&gt;&lt;br&gt;
具体的例子如图所示，类似于给一个表格让模型去填写表格内容。&lt;br&gt;
但当需要提取出的信息较多、句子结构比较复杂的情况下，输入序列会变得非常长：&lt;br&gt;
&lt;img src=&#34;example.jpg&#34; style=&#34;zoom:37%&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;structure-generation-with-uie&#34;&gt;Structure Generation with UIE
&lt;/h3&gt;&lt;p&gt;简单来说，UIE的输入是SSI+Text的形式，输出是用SEL语法表示的结构。UIE整体可以看作是Encoder-Decoder架构的模型：&lt;/p&gt;
&lt;h4 id=&#34;encoder&#34;&gt;Encoder
&lt;/h4&gt;$$
H = Encoder(s_1, s_2, s_{|s|}, x_1, ..., x_{|x|})
$$&lt;h4 id=&#34;decoder&#34;&gt;Decoder
&lt;/h4&gt;$$
y_i, h_i^d = Decoder([H; h_1^d, ..., h_{i-1}^d])
$$&lt;p&gt;
当解码器生成特殊的结束符号&amp;lt;eos&amp;gt;时，解码过程完成。&lt;/p&gt;
&lt;p&gt;作者认为，文本-结构的生成范式将标签视为自然语言标记，这一方法可以有效地转移来自BART、T5等预训练语言模型的知识，相关任务可以很容易地共享知识，因为它们的标签具有相似的语义(例如，location和place)，并共享共同的标签-文本关联(例如，不同事件类型的受害者)。&lt;/p&gt;
&lt;h3 id=&#34;pre-training-for-uie&#34;&gt;Pre-training for UIE
&lt;/h3&gt;&lt;h4 id=&#34;预训练数据集&#34;&gt;预训练数据集
&lt;/h4&gt;&lt;p&gt;包括结构化(例如，知识库)、非结构化(例如，原始文本)和并行(例如，维基百科-维基数据链接)数据：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$D_{record}$(结构)：$D_{record}$是一个结构数据集，每个实例是一个结构化记录$y$。该数据集从ConceptNet(一个常识知识库)和Wikidata中收集结构化记录。$D_{record}$用于预训练UIE的结构解码能力，帮助模型在理解结构化记录后，能够从这些记录生成有效的输出。&lt;/li&gt;
&lt;li&gt;$D_{text}$(文本)：$D_{text}$是一个非结构化文本数据集，包含英语维基百科中的所有纯文本。$D_{text}$用于预训练UIE的语义编码能力。通过对大规模的非结构化文本进行训练，模型能够更好地理解自然语言的语义、上下文和结构特征。&lt;/li&gt;
&lt;li&gt;$D_{pair}$(文本+结构)：$D_{pair}$是文本-结构的并行数据集，每个实例由一对$(token序列x，结构化记录y)$组成。Dpair用于预训练UIE的文本到结构转换能力，使模型能够理解和处理文本与其对应结构之间的关系。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;预训练任务&#34;&gt;预训练任务
&lt;/h4&gt;&lt;p&gt;使用三个序列生成任务来预训练UIE：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Text-to-Structure Pre-training using $D_pair$：$p(y|x,s_meta)$表示在给定文本和模式的条件下生成结构记录$y$的概率。
$$
{L}_{\text {Pair}}=\sum_{(x, y) \in \mathcal{D}_{\text {pair}}}-\log p\left(y \mid x, s_{\text {meta}} ; \theta_{e}, \theta_{d}\right)
$$&lt;/li&gt;
&lt;li&gt;Structure Generation Pre-training with $D_record$：在给定之前生成的token的情况下，预测当前token $y_i$的负对数似然。
$$
{L}_{\text {Record}}=\sum_{y \in \mathcal{D}_{\text {record}}}-\log p\left(y_i \mid y_{&lt; i}; \theta_{d}\right)
$$&lt;/li&gt;
&lt;li&gt;Retrofitting Semantic Representation using $D_text$：给定被Mask的源文本，让模型还原被Mask的部分。
$$
{L}_{\text {Text}}=\sum_{x \in \mathcal{D}_{\text {text}}}-\log p\left(x^{\prime \prime} \mid x^{\prime} ; \theta_{e}, \theta_{d}\right)
$$&lt;/li&gt;
&lt;/ol&gt;
$$
{L}={L}_{\text {Pair}} + {L}_{\text {Record}} + {L}_{\text {Text}}
$$&lt;h3 id=&#34;fine-tuning-for-uie&#34;&gt;Fine-tuning for UIE
&lt;/h3&gt;$$
{L}_{FT}=\sum_{(s, x, y) \in \mathcal{D}_{\text {Task}}}-\log p\left(y \mid x, s; \theta_{e}, \theta_{d}\right)
$$&lt;p&gt;
为了缓解暴露偏差，在解码过程中，作者还设计了一种拒绝机制。给定实例$(s, x, y)$，使用SEL对$y$进行编码，随机加入$[NULL]$噪声。具体示例如图所示：&lt;br&gt;
&lt;img src=&#34;noise_example.jpg&#34; style=&#34;zoom:45%&#34; /&gt;&lt;br&gt;
句子中没有facility实体，在模型学习时随机注入&amp;quot;(facility：[NULL])&amp;ldquo;的噪声。通过这种方式，UIE可以通过生成[NULL]来有效地学习拒绝误导生成。&lt;/p&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment
&lt;/h2&gt;&lt;img src=&#34;main_result.jpg&#34; style=&#34;zoom:58%&#34; /&gt;   
&lt;p&gt;可以看到，UIE在多个任务上SOTA。&lt;br&gt;
ps：其他的实验结果具体可以看原文，在这里就不详细说明了。&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h2&gt;&lt;p&gt;提出了一个统一的文本到结构生成框架——UIE(T5+结构化生成)，它可以通用地建模不同的IE任务，自适应地生成有针对性的结构，并从不同的知识源中学习通用的IE能力：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;多样化的目标结构 &amp;mdash;&amp;gt; SEL统一编码异构抽取结构&lt;/li&gt;
&lt;li&gt;需求特定的抽取模式 &amp;mdash;&amp;gt; 基于模式的提示机制SSI&lt;/li&gt;
&lt;li&gt;不同任务之间知识难以共享 &amp;mdash;&amp;gt; 在包含多个任务的数据集上预训练模型&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
